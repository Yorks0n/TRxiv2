<!DOCTYPE html>
<html lang="en" dir="auto">

<head>
<head>
    <title>radiology and imaging</title>
    <meta charset="utf-8">
    <meta name="description"
        content="Website meta description for google search results go here" />
    <meta name="dc.relation" content="https://trxiv.yorks0n.com" />
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="theme-color" content="#1A94D2" />

    

    
    
    
    <link rel="stylesheet" href="/css/main.min.css" media="screen">

</head>


<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>radiology and imaging | TRxiv2</title>
<meta name="keywords" content="">
<meta name="description" content="A Mutual Knowledge Distillation-Empowered AI Framework for Early Detection of Alzheimer&#39;s Disease Using Incomplete Multi-Modal Images
Authors: Kwak, M. G.; Su, Y.; Chen, K.; Weidman, D.; Wu, T.; Lure, F.; Li, J.
Score: 9.2, Published: 2023-08-25 DOI: 10.1101/2023.08.24.23294574
Early detection of Alzheimers Disease (AD) is crucial to ensure timely interventions and optimize treatment outcomes for patients. While integrating multi-modal neuroimages, such as MRI and PET, has shown great promise, limited research has been done to effectively handle incomplete multi-modal image datasets in the integration.">
<meta name="author" content="">
<link rel="canonical" href="https://trxiv.yorks0n.com/posts/radiology-and-imaging/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.904bd1e751cdd2a584fa6bed3fa1166dfd8ec9949ebfd0c4d69c5add5e17c23d.css" integrity="sha256-kEvR51HN0qWE&#43;mvtP6EWbf2OyZSev9DE1pxa3V4Xwj0=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js" integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG&#43;9vmJ0cTS&#43;ovo0FeA="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://trxiv.yorks0n.com/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://trxiv.yorks0n.com/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://trxiv.yorks0n.com/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://trxiv.yorks0n.com/apple-touch-icon.png">
<link rel="mask-icon" href="https://trxiv.yorks0n.com/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:title" content="radiology and imaging" />
<meta property="og:description" content="A Mutual Knowledge Distillation-Empowered AI Framework for Early Detection of Alzheimer&#39;s Disease Using Incomplete Multi-Modal Images
Authors: Kwak, M. G.; Su, Y.; Chen, K.; Weidman, D.; Wu, T.; Lure, F.; Li, J.
Score: 9.2, Published: 2023-08-25 DOI: 10.1101/2023.08.24.23294574
Early detection of Alzheimers Disease (AD) is crucial to ensure timely interventions and optimize treatment outcomes for patients. While integrating multi-modal neuroimages, such as MRI and PET, has shown great promise, limited research has been done to effectively handle incomplete multi-modal image datasets in the integration." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://trxiv.yorks0n.com/posts/radiology-and-imaging/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-09-13T10:39:03+00:00" />
<meta property="article:modified_time" content="2023-09-13T10:39:03+00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="radiology and imaging"/>
<meta name="twitter:description" content="A Mutual Knowledge Distillation-Empowered AI Framework for Early Detection of Alzheimer&#39;s Disease Using Incomplete Multi-Modal Images
Authors: Kwak, M. G.; Su, Y.; Chen, K.; Weidman, D.; Wu, T.; Lure, F.; Li, J.
Score: 9.2, Published: 2023-08-25 DOI: 10.1101/2023.08.24.23294574
Early detection of Alzheimers Disease (AD) is crucial to ensure timely interventions and optimize treatment outcomes for patients. While integrating multi-modal neuroimages, such as MRI and PET, has shown great promise, limited research has been done to effectively handle incomplete multi-modal image datasets in the integration."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Posts",
      "item": "https://trxiv.yorks0n.com/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  3 ,
      "name": "radiology and imaging",
      "item": "https://trxiv.yorks0n.com/posts/radiology-and-imaging/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "radiology and imaging",
  "name": "radiology and imaging",
  "description": "A Mutual Knowledge Distillation-Empowered AI Framework for Early Detection of Alzheimer\u0026#39;s Disease Using Incomplete Multi-Modal Images\nAuthors: Kwak, M. G.; Su, Y.; Chen, K.; Weidman, D.; Wu, T.; Lure, F.; Li, J.\nScore: 9.2, Published: 2023-08-25 DOI: 10.1101/2023.08.24.23294574\nEarly detection of Alzheimers Disease (AD) is crucial to ensure timely interventions and optimize treatment outcomes for patients. While integrating multi-modal neuroimages, such as MRI and PET, has shown great promise, limited research has been done to effectively handle incomplete multi-modal image datasets in the integration.",
  "keywords": [
    
  ],
  "articleBody": " A Mutual Knowledge Distillation-Empowered AI Framework for Early Detection of Alzheimer's Disease Using Incomplete Multi-Modal Images\nAuthors: Kwak, M. G.; Su, Y.; Chen, K.; Weidman, D.; Wu, T.; Lure, F.; Li, J.\nScore: 9.2, Published: 2023-08-25 DOI: 10.1101/2023.08.24.23294574\nEarly detection of Alzheimers Disease (AD) is crucial to ensure timely interventions and optimize treatment outcomes for patients. While integrating multi-modal neuroimages, such as MRI and PET, has shown great promise, limited research has been done to effectively handle incomplete multi-modal image datasets in the integration. To this end, we propose a deep learning-based framework that employs Mutual Knowledge Distillation (MKD) to jointly model different sub-cohorts based on their respective available image modalities. In MKD, the model with more modalities (e.g., MRI and PET) is considered a teacher while the model with fewer modalities (e.g., only MRI) is considered a student. Our proposed MKD framework includes three key components: First, we design a teacher model that is student-oriented, namely the Student-oriented Multi-modal Teacher (SMT), through multi-modal information disentanglement. Second, we train the student model by not only minimizing its classification errors but also learning from the SMT teacher. Third, we update the teacher model by transfer learning from the students feature extractor because the student model is trained with more samples. Evaluations on Alzheimers Disease Neuroimaging Initiative (ADNI) datasets highlight the effectiveness of our method. Our work demonstrates the potential of using AI for addressing the challenges of incomplete multi-modal neuroimage datasets, opening new avenues for advancing early AD detection and treatment strategies.\nA systematic review of (semi-)automatic quality control of T1-weighted MRI scans\nAuthors: Hendriks, J.; Mutsaerts, H.-J.; Joules, R.; Pena-Nogales, O.; Rodrigues, P. R.; Wolz, R.; Burchell, G. L.; Barkhof, F.; Schrantee, A.\nScore: 1.5, Published: 2023-09-08 DOI: 10.1101/2023.09.07.23295187\nArtifacts in magnetic resonance imaging (MRI) scans degrade image quality and thus negatively affect the outcome measures of clinical and research scanning. Considering the time-consuming and subjective nature of visual quality control (QC), multiple (semi-)automatic QC algorithms have been developed. This systematic review presents an overview of the available (semi-)automatic QC algorithms and software packages designed for raw, structural T1-weighted (T1w) MRI datasets. The objective of this review was to identify the differences among these algorithms in terms of their features of interest, performance, and benchmarks. We queried PubMed, EMBASE (Ovid), and Web of Science databases on the fifth of January 2023, and cross-checked reference lists of retrieved papers. Bias assessment was performed using PROBAST (Prediction model Risk Of Bias ASsessment Tool). A total of 18 distinct algorithms were identified, demonstrating significant variations in methods, features, datasets, and benchmarks. The algorithms were categorized into rule-based, classical machine learning-based, and deep learning-based approaches. Numerous unique features were defined, which can be roughly divided into features capturing entropy, contrast, and normative measures. Due to dataset-specific optimization, it is challenging to draw broad conclusions about comparative performance. Additionally, large variations exist in the used datasets and benchmarks, further hindering direct algorithm comparison. The findings emphasize the need for standardization and comparative studies for advancing QC in MR imaging. Efforts should focus on identifying a dataset-independent measure as well as algorithm-independent methods for assessing the relative performance of different approaches.\nChatGPT 4 Versus ChatGPT 3.5 on The Final FRCR Part A Sample Questions. Assessing Performance and Accuracy of Explanations.\nAuthors: Ghosn, Y.; El Sardouk, O.; Jabbour, Y.; Jrad, M.; Hussein Kamareddine, M.; Abbas, N.; Saade, C.; Abi Ghanem, A.\nScore: 1.2, Published: 2023-09-08 DOI: 10.1101/2023.09.06.23295144\nObjectiveTo evaluate the performance of two versions of ChatGPT, GPT4 and GPT3.5, on the Final FRCR (Part A) also referred to as FRCR Part 2A radiology exam. The primary objective is to assess whether these large language models (LLMs) can effectively answer radiology test questions while providing accurate explanations for the answers. MethodsThe evaluation involves a total of 281 multiple choice questions, combining the 41 FRCR sample questions found on The Royal Collage of Radiologists website and 240 questions from a supplementary test bank. Both GPT4 and GPT3.5 were given the 281 questions with the answer choices, and their responses were assessed for correctness and accuracy of the explanations provided. The 41 FRCR sample questions difficulty was ranked into \"low order\" and \"high order\" questions. A significance level of p\u003c0.05 was used. ResultsGPT4 demonstrated significant improvement over GPT3.5 in answering the 281 questions, achieving 76.5% correct answers compared to 52.7%, respectively (p\u003c0.001). GPT4 demonstrated significant improvement over GPT3.5 in providing accurate explanations for the 41 FRCR sample questions, with an accuracy of 65.9% and 31.7% respectively (p=0.002). The difficulty of the question did not significantly affect the models performances. ConclusionThe findings of this study demonstrate a significant improvement in the performance of GPT4 compared to GPT3.5 on FRCR style examination. However, the accuracy of the provided explanations might limit the models reliability as learning tools. Advances in KnowledgeThe study indirectly explores the potential of LLMs to contribute to the diagnostic accuracy and efficiency of medical imaging while raising questions about the current LLMs limitations in providing reliable explanations for radiology related questions hindering its uses for learning and in clinical practice. HighlightsO_LIChatGPT4 passed an FRCR part 2A style exam while ChatGPT3.5 did not. C_LIO_LIChatGPT4 showed significantly higher correctness of answers and accuracy of explanations. C_LIO_LINo significant difference in performance was observed between \"high order\" and \"lower order\" questions. C_LIO_LIExplanation accuracy was lower than correct answers rate limiting the Models reliability as C_LIO_LIlearning tools. C_LI\n",
  "wordCount" : "894",
  "inLanguage": "en",
  "datePublished": "2023-09-13T10:39:03Z",
  "dateModified": "2023-09-13T10:39:03Z",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://trxiv.yorks0n.com/posts/radiology-and-imaging/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "TRxiv2",
    "logo": {
      "@type": "ImageObject",
      "url": "https://trxiv.yorks0n.com/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://trxiv.yorks0n.com" accesskey="h" title="TRxiv2 (Alt + H)">TRxiv2</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
        </ul>
    </nav>
</header>
<main class="main">
<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title">
      radiology and imaging
    </h1>
    <div class="post-meta"><span>updated on September 13, 2023</span>

</div>
  </header> 
  <div class="post-content"><div class="accordion accordion-flush" id="accordionFlushExample"><div class="accordion-item">
    <h3 class="accordion-header" id="flush-heading10.1101/2023.08.24.23294574">
      <button class="accordion-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapse10.1101/2023.08.24.23294574" aria-expanded="false" aria-controls="flush-collapse10.1101/2023.08.24.23294574">
        <p class="paperTitle">A Mutual Knowledge Distillation-Empowered AI Framework for Early Detection of Alzheimer&#39;s Disease Using Incomplete Multi-Modal Images</p>
      </button>
    </h3>
    <div id="flush-collapse10.1101/2023.08.24.23294574" class="accordion-collapse collapse" aria-labelledby="flush-heading10.1101/2023.08.24.23294574" data-bs-parent="#accordionFlushExample">
      <div class="accordion-body">
        <p class="author">Authors: Kwak, M. G.; Su, Y.; Chen, K.; Weidman, D.; Wu, T.; Lure, F.; Li, J.</p>
        <p class="info">Score: 9.2, Published: 2023-08-25 </p>
        <p class="info">DOI: <a href='https://doi.org/10.1101/2023.08.24.23294574' target='https://doi.org/10.1101/2023.08.24.23294574'> 10.1101/2023.08.24.23294574</a></p>
        <p class="abstract">Early detection of Alzheimers Disease (AD) is crucial to ensure timely interventions and optimize treatment outcomes for patients. While integrating multi-modal neuroimages, such as MRI and PET, has shown great promise, limited research has been done to effectively handle incomplete multi-modal image datasets in the integration. To this end, we propose a deep learning-based framework that employs Mutual Knowledge Distillation (MKD) to jointly model different sub-cohorts based on their respective available image modalities. In MKD, the model with more modalities (e.g., MRI and PET) is considered a teacher while the model with fewer modalities (e.g., only MRI) is considered a student. Our proposed MKD framework includes three key components: First, we design a teacher model that is student-oriented, namely the Student-oriented Multi-modal Teacher (SMT), through multi-modal information disentanglement. Second, we train the student model by not only minimizing its classification errors but also learning from the SMT teacher. Third, we update the teacher model by transfer learning from the students feature extractor because the student model is trained with more samples. Evaluations on Alzheimers Disease Neuroimaging Initiative (ADNI) datasets highlight the effectiveness of our method. Our work demonstrates the potential of using AI for addressing the challenges of incomplete multi-modal neuroimage datasets, opening new avenues for advancing early AD detection and treatment strategies.</p>
      </div>
    </div>
  </div><div class="accordion-item">
    <h3 class="accordion-header" id="flush-heading10.1101/2023.09.07.23295187">
      <button class="accordion-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapse10.1101/2023.09.07.23295187" aria-expanded="false" aria-controls="flush-collapse10.1101/2023.09.07.23295187">
        <p class="paperTitle">A systematic review of (semi-)automatic quality control of T1-weighted MRI scans</p>
      </button>
    </h3>
    <div id="flush-collapse10.1101/2023.09.07.23295187" class="accordion-collapse collapse" aria-labelledby="flush-heading10.1101/2023.09.07.23295187" data-bs-parent="#accordionFlushExample">
      <div class="accordion-body">
        <p class="author">Authors: Hendriks, J.; Mutsaerts, H.-J.; Joules, R.; Pena-Nogales, O.; Rodrigues, P. R.; Wolz, R.; Burchell, G. L.; Barkhof, F.; Schrantee, A.</p>
        <p class="info">Score: 1.5, Published: 2023-09-08 </p>
        <p class="info">DOI: <a href='https://doi.org/10.1101/2023.09.07.23295187' target='https://doi.org/10.1101/2023.09.07.23295187'> 10.1101/2023.09.07.23295187</a></p>
        <p class="abstract">Artifacts in magnetic resonance imaging (MRI) scans degrade image quality and thus negatively affect the outcome measures of clinical and research scanning. Considering the time-consuming and subjective nature of visual quality control (QC), multiple (semi-)automatic QC algorithms have been developed. This systematic review presents an overview of the available (semi-)automatic QC algorithms and software packages designed for raw, structural T1-weighted (T1w) MRI datasets. The objective of this review was to identify the differences among these algorithms in terms of their features of interest, performance, and benchmarks. We queried PubMed, EMBASE (Ovid), and Web of Science databases on the fifth of January 2023, and cross-checked reference lists of retrieved papers. Bias assessment was performed using PROBAST (Prediction model Risk Of Bias ASsessment Tool). A total of 18 distinct algorithms were identified, demonstrating significant variations in methods, features, datasets, and benchmarks. The algorithms were categorized into rule-based, classical machine learning-based, and deep learning-based approaches. Numerous unique features were defined, which can be roughly divided into features capturing entropy, contrast, and normative measures. Due to dataset-specific optimization, it is challenging to draw broad conclusions about comparative performance. Additionally, large variations exist in the used datasets and benchmarks, further hindering direct algorithm comparison. The findings emphasize the need for standardization and comparative studies for advancing QC in MR imaging. Efforts should focus on identifying a dataset-independent measure as well as algorithm-independent methods for assessing the relative performance of different approaches.</p>
      </div>
    </div>
  </div><div class="accordion-item">
    <h3 class="accordion-header" id="flush-heading10.1101/2023.09.06.23295144">
      <button class="accordion-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapse10.1101/2023.09.06.23295144" aria-expanded="false" aria-controls="flush-collapse10.1101/2023.09.06.23295144">
        <p class="paperTitle">ChatGPT 4 Versus ChatGPT 3.5 on The Final FRCR Part A Sample Questions. Assessing Performance and Accuracy of Explanations.</p>
      </button>
    </h3>
    <div id="flush-collapse10.1101/2023.09.06.23295144" class="accordion-collapse collapse" aria-labelledby="flush-heading10.1101/2023.09.06.23295144" data-bs-parent="#accordionFlushExample">
      <div class="accordion-body">
        <p class="author">Authors: Ghosn, Y.; El Sardouk, O.; Jabbour, Y.; Jrad, M.; Hussein Kamareddine, M.; Abbas, N.; Saade, C.; Abi Ghanem, A.</p>
        <p class="info">Score: 1.2, Published: 2023-09-08 </p>
        <p class="info">DOI: <a href='https://doi.org/10.1101/2023.09.06.23295144' target='https://doi.org/10.1101/2023.09.06.23295144'> 10.1101/2023.09.06.23295144</a></p>
        <p class="abstract">ObjectiveTo evaluate the performance of two versions of ChatGPT, GPT4 and GPT3.5, on the Final FRCR (Part A) also referred to as FRCR Part 2A radiology exam. The primary objective is to assess whether these large language models (LLMs) can effectively answer radiology test questions while providing accurate explanations for the answers.

MethodsThe evaluation involves a total of 281 multiple choice questions, combining the 41 FRCR sample questions found on The Royal Collage of Radiologists website and 240 questions from a supplementary test bank. Both GPT4 and GPT3.5 were given the 281 questions with the answer choices, and their responses were assessed for correctness and accuracy of the explanations provided. The 41 FRCR sample questions difficulty was ranked into &#34;low order&#34; and &#34;high order&#34; questions. A significance level of p&lt;0.05 was used.

ResultsGPT4 demonstrated significant improvement over GPT3.5 in answering the 281 questions, achieving 76.5% correct answers compared to 52.7%, respectively (p&lt;0.001). GPT4 demonstrated significant improvement over GPT3.5 in providing accurate explanations for the 41 FRCR sample questions, with an accuracy of 65.9% and 31.7% respectively (p=0.002). The difficulty of the question did not significantly affect the models performances.

ConclusionThe findings of this study demonstrate a significant improvement in the performance of GPT4 compared to GPT3.5 on FRCR style examination. However, the accuracy of the provided explanations might limit the models reliability as learning tools.

Advances in KnowledgeThe study indirectly explores the potential of LLMs to contribute to the diagnostic accuracy and efficiency of medical imaging while raising questions about the current LLMs limitations in providing reliable explanations for radiology related questions hindering its uses for learning and in clinical practice.

HighlightsO_LIChatGPT4 passed an FRCR part 2A style exam while ChatGPT3.5 did not.
C_LIO_LIChatGPT4 showed significantly higher correctness of answers and accuracy of explanations.
C_LIO_LINo significant difference in performance was observed between &#34;high order&#34; and &#34;lower order&#34; questions.
C_LIO_LIExplanation accuracy was lower than correct answers rate limiting the Models reliability as
C_LIO_LIlearning tools.
C_LI</p>
      </div>
    </div>
  </div>
</div>









<script src="/js/bundle.min.js" defer></script>



  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2023 <a href="https://trxiv.yorks0n.com">TRxiv2</a></span>
    <span>
        · Made by Yorkson
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
