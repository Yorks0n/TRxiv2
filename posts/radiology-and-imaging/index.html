<!DOCTYPE html>
<html lang="en" dir="auto">

<head>
<head>
    <title>radiology and imaging</title>
    <meta charset="utf-8">
    <meta name="description"
        content="Website meta description for google search results go here" />
    <meta name="dc.relation" content="https://trxiv.yorks0n.com" />
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="theme-color" content="#1A94D2" />

    

    
    
    
    <link rel="stylesheet" href="/css/main.min.css" media="screen">

</head>


<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>radiology and imaging | TRxiv2</title>
<meta name="keywords" content="">
<meta name="description" content="Multi-contrast high-field quality image synthesis for portable low-field MRI using generative adversarial networks and paired data
Authors: Lucas, A.; Arnold, T. C.; Okar, S. V.; Vadali, C.; Kawatra, K. D.; Ren, Z.; Cao, Q.; Shinohara, R. T.; Schindler, M. K.; Davis, K. A.; Litt, B.; Reich, D. S.; Stein, J. M.
Score: 18.3, Published: 2023-12-29 DOI: 10.1101/2023.12.28.23300409
IntroductionPortable low-field strength (64mT) MRI scanners promise to increase access to neuroimaging for clinical and research purposes, however these devices produce lower quality images compared to high-field scanners.">
<meta name="author" content="">
<link rel="canonical" href="https://trxiv.yorks0n.com/posts/radiology-and-imaging/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.904bd1e751cdd2a584fa6bed3fa1166dfd8ec9949ebfd0c4d69c5add5e17c23d.css" integrity="sha256-kEvR51HN0qWE&#43;mvtP6EWbf2OyZSev9DE1pxa3V4Xwj0=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js" integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG&#43;9vmJ0cTS&#43;ovo0FeA="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://trxiv.yorks0n.com/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://trxiv.yorks0n.com/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://trxiv.yorks0n.com/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://trxiv.yorks0n.com/apple-touch-icon.png">
<link rel="mask-icon" href="https://trxiv.yorks0n.com/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:title" content="radiology and imaging" />
<meta property="og:description" content="Multi-contrast high-field quality image synthesis for portable low-field MRI using generative adversarial networks and paired data
Authors: Lucas, A.; Arnold, T. C.; Okar, S. V.; Vadali, C.; Kawatra, K. D.; Ren, Z.; Cao, Q.; Shinohara, R. T.; Schindler, M. K.; Davis, K. A.; Litt, B.; Reich, D. S.; Stein, J. M.
Score: 18.3, Published: 2023-12-29 DOI: 10.1101/2023.12.28.23300409
IntroductionPortable low-field strength (64mT) MRI scanners promise to increase access to neuroimaging for clinical and research purposes, however these devices produce lower quality images compared to high-field scanners." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://trxiv.yorks0n.com/posts/radiology-and-imaging/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-01-21T10:39:08+00:00" />
<meta property="article:modified_time" content="2024-01-21T10:39:08+00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="radiology and imaging"/>
<meta name="twitter:description" content="Multi-contrast high-field quality image synthesis for portable low-field MRI using generative adversarial networks and paired data
Authors: Lucas, A.; Arnold, T. C.; Okar, S. V.; Vadali, C.; Kawatra, K. D.; Ren, Z.; Cao, Q.; Shinohara, R. T.; Schindler, M. K.; Davis, K. A.; Litt, B.; Reich, D. S.; Stein, J. M.
Score: 18.3, Published: 2023-12-29 DOI: 10.1101/2023.12.28.23300409
IntroductionPortable low-field strength (64mT) MRI scanners promise to increase access to neuroimaging for clinical and research purposes, however these devices produce lower quality images compared to high-field scanners."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Posts",
      "item": "https://trxiv.yorks0n.com/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  3 ,
      "name": "radiology and imaging",
      "item": "https://trxiv.yorks0n.com/posts/radiology-and-imaging/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "radiology and imaging",
  "name": "radiology and imaging",
  "description": "Multi-contrast high-field quality image synthesis for portable low-field MRI using generative adversarial networks and paired data\nAuthors: Lucas, A.; Arnold, T. C.; Okar, S. V.; Vadali, C.; Kawatra, K. D.; Ren, Z.; Cao, Q.; Shinohara, R. T.; Schindler, M. K.; Davis, K. A.; Litt, B.; Reich, D. S.; Stein, J. M.\nScore: 18.3, Published: 2023-12-29 DOI: 10.1101/2023.12.28.23300409\nIntroductionPortable low-field strength (64mT) MRI scanners promise to increase access to neuroimaging for clinical and research purposes, however these devices produce lower quality images compared to high-field scanners.",
  "keywords": [
    
  ],
  "articleBody": " Multi-contrast high-field quality image synthesis for portable low-field MRI using generative adversarial networks and paired data\nAuthors: Lucas, A.; Arnold, T. C.; Okar, S. V.; Vadali, C.; Kawatra, K. D.; Ren, Z.; Cao, Q.; Shinohara, R. T.; Schindler, M. K.; Davis, K. A.; Litt, B.; Reich, D. S.; Stein, J. M.\nScore: 18.3, Published: 2023-12-29 DOI: 10.1101/2023.12.28.23300409\nIntroductionPortable low-field strength (64mT) MRI scanners promise to increase access to neuroimaging for clinical and research purposes, however these devices produce lower quality images compared to high-field scanners. In this study, we developed and evaluated a deep learning architecture to generate high-field quality brain images from low-field inputs using a paired dataset of multiple sclerosis (MS) patients scanned at 64mT and 3T. MethodsA total of 49 MS patients were scanned on portable 64mT and standard 3T scanners at Penn (n=25) or the National Institutes of Health (NIH, n=24) with T1-weighted, T2-weighted and FLAIR acquisitions. Using this paired data, we developed a generative adversarial network (GAN) architecture for low- to high-field image translation (LowGAN). We then evaluated synthesized images with respect to image quality, brain morphometry, and white matter lesions. ResultsSynthetic high-field images demonstrated visually superior quality compared to low-field inputs and significantly higher normalized cross-correlation (NCC) to actual high-field images for T1 (p=0.001) and FLAIR (p\u003c0.001) contrasts. LowGAN generally outperformed the current state- of-the-art for low-field volumetrics. For example, thalamic, lateral ventricle, and total cortical volumes in LowGAN outputs did not differ significantly from 3T measurements. Synthetic outputs preserved MS lesions and captured a known inverse relationship between total lesion volume and thalamic volume. ConclusionsLowGAN generates synthetic high-field images with comparable visual and quantitative quality to actual high-field scans. Enhancing portable MRI image quality could add value and boost clinician confidence, enabling wider adoption of this technology.\nFive dominant dimensions of brain aging are identified via deep learning: associations with clinical, lifestyle, and genetic measures\nAuthors: Yang, Z.; Wen, J.; Erus, G.; Govindarajan, S. T.; Melhem, R.; Mamourian, E.; Cui, Y.; Srinivasan, D.; Abdulkadir, A.; Parmpi, P.; Wittfeld, K.; Grabe, H. J.; Bulow, R.; Frenzel, S.; Tosun, D.; Bilgel, M.; An, Y.; Yi, D.; Marcus, D. S.; LaMontagne, P.; Benzinger, T. L. S.; Heckbert, S. R.; Austin, T. R.; Waldstein, S. R.; Evans, M. K.; Zonderman, A. B.; Launer, L. J.; Sotiras, A.; Espeland, M. A.; Masters, C. L.; Maruff, P.; Fripp, J.; Toga, A.; O'Bryant, S.; Chakravarty, M. M.; Villeneuve, S.; Johnson, S. C.; Morris, J. C.; Albert, M. S.; Yaffe, K.; Volzke, H.; Ferrucci, L.; Bryan, N. R.; Shin\nScore: 11.1, Published: 2023-12-30 DOI: 10.1101/2023.12.29.23300642\nAbstractBrain aging is a complex process influenced by various lifestyle, environmental, and genetic factors, as well as by age-related and often co-existing pathologies. MRI and, more recently, AI methods have been instrumental in understanding the neuroanatomical changes that occur during aging in large and diverse populations. However, the multiplicity and mutual overlap of both pathologic processes and affected brain regions make it difficult to precisely characterize the underlying neurodegenerative profile of an individual from an MRI scan. Herein, we leverage a state-of-the art deep representation learning method, Surreal-GAN, and present both methodological advances and extensive experimental results that allow us to elucidate the heterogeneity of brain aging in a large and diverse cohort of 49,482 individuals from 11 studies. Five dominant patterns of neurodegeneration were identified and quantified for each individual by their respective (herein referred to as) R-indices. Significant associations between R-indices and distinct biomedical, lifestyle, and genetic factors provide insights into the etiology of observed variances. Furthermore, baseline R-indices showed predictive value for disease progression and mortality. These five R-indices contribute to MRI-based precision diagnostics, prognostication, and may inform stratification into clinical trials.\nCoronary Artery Calcium Scans Powered by Artificial Intelligence Predicts Atrial Fibrillation Comparably to Cardiac Magnetic Resonance Imaging: The Multi-Ethnic Study of Atherosclerosis (MESA)\nAuthors: Naghavi, M.; Reeves, A. P.; Atlas, K. C.; LI, D.; Goodarzynejad, H.; Zhang, C.; Atlas, T. L.; Henschke, C.; Budoff, M. J.; Yankelevitz, D.\nScore: 1.4, Published: 2024-01-04 DOI: 10.1101/2024.01.04.24300746\nBackgroundApplying artificial intelligence to coronary artery calcium computed tomography scan (AI-CAC) provides more actionable information beyond the Agatston coronary artery calcium (CAC) score. We have recently shown that AI-CAC automated left atrial (LA) volumetry enabled prediction of atrial fibrillation (AF) in as early as one year. In this study we evaluated the performance of AI-CAC automated LA volumetry versus LA volume measured by human experts using cardiac magnetic resonance imaging (CMRI) for predicting AF, and compared them with CHARGE-AF risk score, Agatston score, and NT-proBNP (BNP). MethodsWe used 15-year outcome data from 3552 asymptomatic individuals (52.2% women, ages 45-84 years) who underwent both CAC scans and CMRI in the baseline examination (2000-2002) of the Multi-Ethnic Study of Atherosclerosis (MESA). AI-CAC took on average 21 seconds per scan. CMRI LA volume was previously measured by human experts. Data on BNP, CHARGE-AF risk score and the Agatston score were obtained from MESA. ResultsOver 15 years follow-up, 562 cases of AF accrued. The ROC AUC for AI-CAC versus CMRI and CHARGE-AF were not significantly different (AUC 0.807, 0.808, 0.800 respectively, p=0.60). The AUC for BNP (0.707) and Agatston score (0.694) were significantly lower than the rest (p\u003c.0001). AI-CAC and CMRI significantly improved the continuous Net Reclassification Index (NRI) for prediction of AF when added to CHARGE-AF risk score (0.28, 0.31), BNP (0.43, 0.32), and Agatston score (0.69, 0.41) respectively (p for all\u003c0.0001). ConclusionAI-CAC automated LA volumetry and CMRI LA volume measured by human experts similarly predicted incident AF over 15 years.\nA Comparative Study: Diagnostic Performance of ChatGPT 3.5, Google Bard, Microsoft Bing, and Radiologists in Thoracic Radiology Cases\nAuthors: Gunes, Y. C.; Cesur, T.\nScore: 0.8, Published: 2024-01-20 DOI: 10.1101/2024.01.18.24301495\nPurpose: To investigate and compare the diagnostic performance of ChatGPT 3.5, Google Bard, Microsoft Bing, and two board-certified radiologists in thoracic radiology cases published by The Society of Thoracic Radiology. Materials and Methods: We collected 124 \"Case of the Month\" from the Society of Thoracic Radiology website between March 2012 and December 2023. Medical history and imaging findings were input into ChatGPT 3.5, Google Bard, and Microsoft Bing for diagnosis and differential diagnosis. Two board-certified radiologists provided their diagnoses. Cases were categorized anatomically (parenchyma, airways, mediastinum-pleura-chest wall, and vascular) and further classified as specific or non-specific for radiological diagnosis. Diagnostic accuracy and differential diagnosis scores were analyzed using chi-square, Kruskal-Wallis and Mann-Whitney U tests. Results: Among 124 cases, ChatGPT demonstrated the highest diagnostic accuracy (53.2%), outperforming radiologists (52.4% and 41.1%), Bard (33.1%), and Bing (29.8%). Specific cases revealed varying diagnostic accuracies, with Radiologist I achieving (65.6%), surpassing ChatGPT (63.5%), Radiologist II (52.0%), Bard (39.5%), and Bing (35.4%). ChatGPT 3.5 and Bing had higher differential scores in specific cases (P\u003c0.05), whereas Bard did not (P=0.114). All three had a higher diagnostic accuracy in specific cases (P\u003c0.05). No differences were found in the diagnostic accuracy or differential diagnosis scores of the four anatomical location (P\u003e0.05). Conclusion: ChatGPT 3.5 demonstrated higher diagnostic accuracy than Bing, Bard and radiologists in text-based thoracic radiology cases. Large language models hold great promise in this field under proper medical supervision.\nCross-Center Validation of Deep Learning Model for Musculoskeletal Fracture Detection in Radiographic Imaging: A Feasibility Study\nAuthors: Hruby, R.; Kvak, D.; Dandar, J.; Atakhanova, A.; Misar, M.; Dufek, D.\nScore: 0.5, Published: 2024-01-17 DOI: 10.1101/2024.01.17.24301244\nFractures, often resulting from trauma, overuse, or osteoporosis, pose diagnostic challenges due to their variable clinical manifestations. To address this, we propose a deep learning-based decision support system to enhance the efficacy of fracture detection in radiographic imaging. For the purpose of our study, we utilized 720 annotated musculoskeletal (MSK) X-rays from the MURA dataset, augmented by bounding box-level annotation, for training the YOLO (You Only Look Once) model. The models performance was subsequently tested on two datasets, sampled FracAtlas dataset (Dataset 1, 840 images, nNORMAL = 696, nFRACTURE = 144) and own internal dataset (Dataset 2, 124 images, nNORMAL = 50, nFRACTURE = 74), encompassing a diverse range of MSK radiographs. The results showed a Sensitivity (Se) of 0.910 (95% CI: 0.852-0.946) and Specificity (Sp) of 0.557 (95% CI: 0.520-0.594) on the Dataset 1, and a Se of 0.622 (95% CI: 0.508-0.724) and Sp of 0.740 (95% CI: 0.604-0.841) on the Dataset 2. This study underscores the promising role of AI in medical imaging, providing a solid foundation for future research and advancements in the field of radiographic diagnostics.\n",
  "wordCount" : "1389",
  "inLanguage": "en",
  "datePublished": "2024-01-21T10:39:08Z",
  "dateModified": "2024-01-21T10:39:08Z",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://trxiv.yorks0n.com/posts/radiology-and-imaging/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "TRxiv2",
    "logo": {
      "@type": "ImageObject",
      "url": "https://trxiv.yorks0n.com/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://trxiv.yorks0n.com" accesskey="h" title="TRxiv2 (Alt + H)">TRxiv2</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
        </ul>
    </nav>
</header>
<main class="main">
<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title">
      radiology and imaging
    </h1>
    <div class="post-meta">&lt;span&gt;updated on January 21, 2024&lt;/span&gt;

</div>
  </header> 
  <div class="post-content"><div class="accordion accordion-flush" id="accordionFlushExample"><div class="accordion-item">
    <h3 class="accordion-header" id="flush-heading10.1101/2023.12.28.23300409">
      <button class="accordion-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapse10.1101/2023.12.28.23300409" aria-expanded="false" aria-controls="flush-collapse10.1101/2023.12.28.23300409">
        <p class="paperTitle">Multi-contrast high-field quality image synthesis for portable low-field MRI using generative adversarial networks and paired data</p>
      </button>
    </h3>
    <div id="flush-collapse10.1101/2023.12.28.23300409" class="accordion-collapse collapse" aria-labelledby="flush-heading10.1101/2023.12.28.23300409" data-bs-parent="#accordionFlushExample">
      <div class="accordion-body">
        <p class="author">Authors: Lucas, A.; Arnold, T. C.; Okar, S. V.; Vadali, C.; Kawatra, K. D.; Ren, Z.; Cao, Q.; Shinohara, R. T.; Schindler, M. K.; Davis, K. A.; Litt, B.; Reich, D. S.; Stein, J. M.</p>
        <p class="info">Score: 18.3, Published: 2023-12-29 </p>
        <p class="info">DOI: <a href='https://doi.org/10.1101/2023.12.28.23300409' target='https://doi.org/10.1101/2023.12.28.23300409'> 10.1101/2023.12.28.23300409</a></p>
        <p class="abstract">IntroductionPortable low-field strength (64mT) MRI scanners promise to increase access to neuroimaging for clinical and research purposes, however these devices produce lower quality images compared to high-field scanners. In this study, we developed and evaluated a deep learning architecture to generate high-field quality brain images from low-field inputs using a paired dataset of multiple sclerosis (MS) patients scanned at 64mT and 3T.

MethodsA total of 49 MS patients were scanned on portable 64mT and standard 3T scanners at Penn (n=25) or the National Institutes of Health (NIH, n=24) with T1-weighted, T2-weighted and FLAIR acquisitions. Using this paired data, we developed a generative adversarial network (GAN) architecture for low- to high-field image translation (LowGAN). We then evaluated synthesized images with respect to image quality, brain morphometry, and white matter lesions.

ResultsSynthetic high-field images demonstrated visually superior quality compared to low-field inputs and significantly higher normalized cross-correlation (NCC) to actual high-field images for T1 (p=0.001) and FLAIR (p&lt;0.001) contrasts. LowGAN generally outperformed the current state- of-the-art for low-field volumetrics. For example, thalamic, lateral ventricle, and total cortical volumes in LowGAN outputs did not differ significantly from 3T measurements. Synthetic outputs preserved MS lesions and captured a known inverse relationship between total lesion volume and thalamic volume.

ConclusionsLowGAN generates synthetic high-field images with comparable visual and quantitative quality to actual high-field scans. Enhancing portable MRI image quality could add value and boost clinician confidence, enabling wider adoption of this technology.</p>
      </div>
    </div>
  </div><div class="accordion-item">
    <h3 class="accordion-header" id="flush-heading10.1101/2023.12.29.23300642">
      <button class="accordion-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapse10.1101/2023.12.29.23300642" aria-expanded="false" aria-controls="flush-collapse10.1101/2023.12.29.23300642">
        <p class="paperTitle">Five dominant dimensions of brain aging are identified via deep learning: associations with clinical, lifestyle, and genetic measures</p>
      </button>
    </h3>
    <div id="flush-collapse10.1101/2023.12.29.23300642" class="accordion-collapse collapse" aria-labelledby="flush-heading10.1101/2023.12.29.23300642" data-bs-parent="#accordionFlushExample">
      <div class="accordion-body">
        <p class="author">Authors: Yang, Z.; Wen, J.; Erus, G.; Govindarajan, S. T.; Melhem, R.; Mamourian, E.; Cui, Y.; Srinivasan, D.; Abdulkadir, A.; Parmpi, P.; Wittfeld, K.; Grabe, H. J.; Bulow, R.; Frenzel, S.; Tosun, D.; Bilgel, M.; An, Y.; Yi, D.; Marcus, D. S.; LaMontagne, P.; Benzinger, T. L. S.; Heckbert, S. R.; Austin, T. R.; Waldstein, S. R.; Evans, M. K.; Zonderman, A. B.; Launer, L. J.; Sotiras, A.; Espeland, M. A.; Masters, C. L.; Maruff, P.; Fripp, J.; Toga, A.; O&#39;Bryant, S.; Chakravarty, M. M.; Villeneuve, S.; Johnson, S. C.; Morris, J. C.; Albert, M. S.; Yaffe, K.; Volzke, H.; Ferrucci, L.; Bryan, N. R.; Shin</p>
        <p class="info">Score: 11.1, Published: 2023-12-30 </p>
        <p class="info">DOI: <a href='https://doi.org/10.1101/2023.12.29.23300642' target='https://doi.org/10.1101/2023.12.29.23300642'> 10.1101/2023.12.29.23300642</a></p>
        <p class="abstract">AbstractBrain aging is a complex process influenced by various lifestyle, environmental, and genetic factors, as well as by age-related and often co-existing pathologies. MRI and, more recently, AI methods have been instrumental in understanding the neuroanatomical changes that occur during aging in large and diverse populations. However, the multiplicity and mutual overlap of both pathologic processes and affected brain regions make it difficult to precisely characterize the underlying neurodegenerative profile of an individual from an MRI scan. Herein, we leverage a state-of-the art deep representation learning method, Surreal-GAN, and present both methodological advances and extensive experimental results that allow us to elucidate the heterogeneity of brain aging in a large and diverse cohort of 49,482 individuals from 11 studies. Five dominant patterns of neurodegeneration were identified and quantified for each individual by their respective (herein referred to as) R-indices. Significant associations between R-indices and distinct biomedical, lifestyle, and genetic factors provide insights into the etiology of observed variances. Furthermore, baseline R-indices showed predictive value for disease progression and mortality. These five R-indices contribute to MRI-based precision diagnostics, prognostication, and may inform stratification into clinical trials.</p>
      </div>
    </div>
  </div><div class="accordion-item">
    <h3 class="accordion-header" id="flush-heading10.1101/2024.01.04.24300746">
      <button class="accordion-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapse10.1101/2024.01.04.24300746" aria-expanded="false" aria-controls="flush-collapse10.1101/2024.01.04.24300746">
        <p class="paperTitle">Coronary Artery Calcium Scans Powered by Artificial Intelligence Predicts Atrial Fibrillation Comparably to Cardiac Magnetic Resonance Imaging: The Multi-Ethnic Study of Atherosclerosis (MESA)</p>
      </button>
    </h3>
    <div id="flush-collapse10.1101/2024.01.04.24300746" class="accordion-collapse collapse" aria-labelledby="flush-heading10.1101/2024.01.04.24300746" data-bs-parent="#accordionFlushExample">
      <div class="accordion-body">
        <p class="author">Authors: Naghavi, M.; Reeves, A. P.; Atlas, K. C.; LI, D.; Goodarzynejad, H.; Zhang, C.; Atlas, T. L.; Henschke, C.; Budoff, M. J.; Yankelevitz, D.</p>
        <p class="info">Score: 1.4, Published: 2024-01-04 </p>
        <p class="info">DOI: <a href='https://doi.org/10.1101/2024.01.04.24300746' target='https://doi.org/10.1101/2024.01.04.24300746'> 10.1101/2024.01.04.24300746</a></p>
        <p class="abstract">BackgroundApplying artificial intelligence to coronary artery calcium computed tomography scan (AI-CAC) provides more actionable information beyond the Agatston coronary artery calcium (CAC) score. We have recently shown that AI-CAC automated left atrial (LA) volumetry enabled prediction of atrial fibrillation (AF) in as early as one year. In this study we evaluated the performance of AI-CAC automated LA volumetry versus LA volume measured by human experts using cardiac magnetic resonance imaging (CMRI) for predicting AF, and compared them with CHARGE-AF risk score, Agatston score, and NT-proBNP (BNP).

MethodsWe used 15-year outcome data from 3552 asymptomatic individuals (52.2% women, ages 45-84 years) who underwent both CAC scans and CMRI in the baseline examination (2000-2002) of the Multi-Ethnic Study of Atherosclerosis (MESA). AI-CAC took on average 21 seconds per scan. CMRI LA volume was previously measured by human experts. Data on BNP, CHARGE-AF risk score and the Agatston score were obtained from MESA.

ResultsOver 15 years follow-up, 562 cases of AF accrued. The ROC AUC for AI-CAC versus CMRI and CHARGE-AF were not significantly different (AUC 0.807, 0.808, 0.800 respectively, p=0.60). The AUC for BNP (0.707) and Agatston score (0.694) were significantly lower than the rest (p&lt;.0001). AI-CAC and CMRI significantly improved the continuous Net Reclassification Index (NRI) for prediction of AF when added to CHARGE-AF risk score (0.28, 0.31), BNP (0.43, 0.32), and Agatston score (0.69, 0.41) respectively (p for all&lt;0.0001).

ConclusionAI-CAC automated LA volumetry and CMRI LA volume measured by human experts similarly predicted incident AF over 15 years.</p>
      </div>
    </div>
  </div><div class="accordion-item">
    <h3 class="accordion-header" id="flush-heading10.1101/2024.01.18.24301495">
      <button class="accordion-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapse10.1101/2024.01.18.24301495" aria-expanded="false" aria-controls="flush-collapse10.1101/2024.01.18.24301495">
        <p class="paperTitle">A Comparative Study: Diagnostic Performance of ChatGPT 3.5, Google Bard, Microsoft Bing, and Radiologists in Thoracic Radiology Cases</p>
      </button>
    </h3>
    <div id="flush-collapse10.1101/2024.01.18.24301495" class="accordion-collapse collapse" aria-labelledby="flush-heading10.1101/2024.01.18.24301495" data-bs-parent="#accordionFlushExample">
      <div class="accordion-body">
        <p class="author">Authors: Gunes, Y. C.; Cesur, T.</p>
        <p class="info">Score: 0.8, Published: 2024-01-20 </p>
        <p class="info">DOI: <a href='https://doi.org/10.1101/2024.01.18.24301495' target='https://doi.org/10.1101/2024.01.18.24301495'> 10.1101/2024.01.18.24301495</a></p>
        <p class="abstract">Purpose: To investigate and compare the diagnostic performance of ChatGPT 3.5, Google Bard, Microsoft Bing, and two board-certified radiologists in thoracic radiology cases published by The Society of Thoracic Radiology. Materials and Methods: We collected 124 &#34;Case of the Month&#34; from the Society of Thoracic Radiology website between March 2012 and December 2023. Medical history and imaging findings were input into ChatGPT 3.5, Google Bard, and Microsoft Bing for diagnosis and differential diagnosis. Two board-certified radiologists provided their diagnoses. Cases were categorized anatomically (parenchyma, airways, mediastinum-pleura-chest wall, and vascular) and further classified as specific or non-specific for radiological diagnosis. Diagnostic accuracy and differential diagnosis scores were analyzed using chi-square, Kruskal-Wallis and Mann-Whitney U tests. Results: Among 124 cases, ChatGPT demonstrated the highest diagnostic accuracy (53.2%), outperforming radiologists (52.4% and 41.1%), Bard (33.1%), and Bing (29.8%). Specific cases revealed varying diagnostic accuracies, with Radiologist I achieving (65.6%), surpassing ChatGPT (63.5%), Radiologist II (52.0%), Bard (39.5%), and Bing (35.4%). ChatGPT 3.5 and Bing had higher differential scores in specific cases (P&lt;0.05), whereas Bard did not (P=0.114). All three had a higher diagnostic accuracy in specific cases (P&lt;0.05). No differences were found in the diagnostic accuracy or differential diagnosis scores of the four anatomical location (P&gt;0.05). Conclusion: ChatGPT 3.5 demonstrated higher diagnostic accuracy than Bing, Bard and radiologists in text-based thoracic radiology cases. Large language models hold great promise in this field under proper medical supervision.</p>
      </div>
    </div>
  </div><div class="accordion-item">
    <h3 class="accordion-header" id="flush-heading10.1101/2024.01.17.24301244">
      <button class="accordion-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapse10.1101/2024.01.17.24301244" aria-expanded="false" aria-controls="flush-collapse10.1101/2024.01.17.24301244">
        <p class="paperTitle">Cross-Center Validation of Deep Learning Model for Musculoskeletal Fracture Detection in Radiographic Imaging: A Feasibility Study</p>
      </button>
    </h3>
    <div id="flush-collapse10.1101/2024.01.17.24301244" class="accordion-collapse collapse" aria-labelledby="flush-heading10.1101/2024.01.17.24301244" data-bs-parent="#accordionFlushExample">
      <div class="accordion-body">
        <p class="author">Authors: Hruby, R.; Kvak, D.; Dandar, J.; Atakhanova, A.; Misar, M.; Dufek, D.</p>
        <p class="info">Score: 0.5, Published: 2024-01-17 </p>
        <p class="info">DOI: <a href='https://doi.org/10.1101/2024.01.17.24301244' target='https://doi.org/10.1101/2024.01.17.24301244'> 10.1101/2024.01.17.24301244</a></p>
        <p class="abstract">Fractures, often resulting from trauma, overuse, or osteoporosis, pose diagnostic challenges due to their variable clinical manifestations. To address this, we propose a deep learning-based decision support system to enhance the efficacy of fracture detection in radiographic imaging. For the purpose of our study, we utilized 720 annotated musculoskeletal (MSK) X-rays from the MURA dataset, augmented by bounding box-level annotation, for training the YOLO (You Only Look Once) model. The models performance was subsequently tested on two datasets, sampled FracAtlas dataset (Dataset 1, 840 images, nNORMAL = 696, nFRACTURE = 144) and own internal dataset (Dataset 2, 124 images, nNORMAL = 50, nFRACTURE = 74), encompassing a diverse range of MSK radiographs. The results showed a Sensitivity (Se) of 0.910 (95% CI: 0.852-0.946) and Specificity (Sp) of 0.557 (95% CI: 0.520-0.594) on the Dataset 1, and a Se of 0.622 (95% CI: 0.508-0.724) and Sp of 0.740 (95% CI: 0.604-0.841) on the Dataset 2. This study underscores the promising role of AI in medical imaging, providing a solid foundation for future research and advancements in the field of radiographic diagnostics.</p>
      </div>
    </div>
  </div>
</div>









<script src="/js/bundle.min.js" defer></script>



  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2024 <a href="https://trxiv.yorks0n.com">TRxiv2</a></span>
    <span>
        · Made by Yorkson
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
