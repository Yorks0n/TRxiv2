<!DOCTYPE html>
<html lang="en" dir="auto">

<head>
<head>
    <title>radiology and imaging</title>
    <meta charset="utf-8">
    <meta name="description"
        content="Website meta description for google search results go here" />
    <meta name="dc.relation" content="https://trxiv.yorks0n.com" />
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="theme-color" content="#1A94D2" />

    

    
    
    
    <link rel="stylesheet" href="/css/main.min.css" media="screen">

</head>


<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>radiology and imaging | TRxiv2</title>
<meta name="keywords" content="">
<meta name="description" content="Spot the Difference: Can ChatGPT4-Vision Transform Radiology Artificial Intelligence?
Authors: Kelly, B. S. S.; Duignan, S.; Mathur, P.; Dillon, H.; Lee, E.; Yeom, K. W.; Keane, P.; Killeen, R. P.; Lawlor, A.
Score: 3.8, Published: 2023-11-18 DOI: 10.1101/2023.11.15.23298499
OpenAIs flagship Large Language Model ChatGPT can now accept image input (GPT4V). &#34;Spot the Difference&#34; and &#34;Medical&#34; have been suggested as emerging applications. The interpretation of medical images is a dynamic process not a static task.">
<meta name="author" content="">
<link rel="canonical" href="https://trxiv.yorks0n.com/posts/radiology-and-imaging/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.904bd1e751cdd2a584fa6bed3fa1166dfd8ec9949ebfd0c4d69c5add5e17c23d.css" integrity="sha256-kEvR51HN0qWE&#43;mvtP6EWbf2OyZSev9DE1pxa3V4Xwj0=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js" integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG&#43;9vmJ0cTS&#43;ovo0FeA="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://trxiv.yorks0n.com/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://trxiv.yorks0n.com/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://trxiv.yorks0n.com/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://trxiv.yorks0n.com/apple-touch-icon.png">
<link rel="mask-icon" href="https://trxiv.yorks0n.com/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:title" content="radiology and imaging" />
<meta property="og:description" content="Spot the Difference: Can ChatGPT4-Vision Transform Radiology Artificial Intelligence?
Authors: Kelly, B. S. S.; Duignan, S.; Mathur, P.; Dillon, H.; Lee, E.; Yeom, K. W.; Keane, P.; Killeen, R. P.; Lawlor, A.
Score: 3.8, Published: 2023-11-18 DOI: 10.1101/2023.11.15.23298499
OpenAIs flagship Large Language Model ChatGPT can now accept image input (GPT4V). &#34;Spot the Difference&#34; and &#34;Medical&#34; have been suggested as emerging applications. The interpretation of medical images is a dynamic process not a static task." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://trxiv.yorks0n.com/posts/radiology-and-imaging/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-11-29T10:40:14+00:00" />
<meta property="article:modified_time" content="2023-11-29T10:40:14+00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="radiology and imaging"/>
<meta name="twitter:description" content="Spot the Difference: Can ChatGPT4-Vision Transform Radiology Artificial Intelligence?
Authors: Kelly, B. S. S.; Duignan, S.; Mathur, P.; Dillon, H.; Lee, E.; Yeom, K. W.; Keane, P.; Killeen, R. P.; Lawlor, A.
Score: 3.8, Published: 2023-11-18 DOI: 10.1101/2023.11.15.23298499
OpenAIs flagship Large Language Model ChatGPT can now accept image input (GPT4V). &#34;Spot the Difference&#34; and &#34;Medical&#34; have been suggested as emerging applications. The interpretation of medical images is a dynamic process not a static task."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Posts",
      "item": "https://trxiv.yorks0n.com/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  3 ,
      "name": "radiology and imaging",
      "item": "https://trxiv.yorks0n.com/posts/radiology-and-imaging/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "radiology and imaging",
  "name": "radiology and imaging",
  "description": "Spot the Difference: Can ChatGPT4-Vision Transform Radiology Artificial Intelligence?\nAuthors: Kelly, B. S. S.; Duignan, S.; Mathur, P.; Dillon, H.; Lee, E.; Yeom, K. W.; Keane, P.; Killeen, R. P.; Lawlor, A.\nScore: 3.8, Published: 2023-11-18 DOI: 10.1101/2023.11.15.23298499\nOpenAIs flagship Large Language Model ChatGPT can now accept image input (GPT4V). \u0026#34;Spot the Difference\u0026#34; and \u0026#34;Medical\u0026#34; have been suggested as emerging applications. The interpretation of medical images is a dynamic process not a static task.",
  "keywords": [
    
  ],
  "articleBody": " Spot the Difference: Can ChatGPT4-Vision Transform Radiology Artificial Intelligence?\nAuthors: Kelly, B. S. S.; Duignan, S.; Mathur, P.; Dillon, H.; Lee, E.; Yeom, K. W.; Keane, P.; Killeen, R. P.; Lawlor, A.\nScore: 3.8, Published: 2023-11-18 DOI: 10.1101/2023.11.15.23298499\nOpenAIs flagship Large Language Model ChatGPT can now accept image input (GPT4V). \"Spot the Difference\" and \"Medical\" have been suggested as emerging applications. The interpretation of medical images is a dynamic process not a static task. Diagnosis and treatment of Multiple Sclerosis is dependent on identification of radiologic change. We aimed to compare the zero-shot performance of GPT4V to a trained U-Net and Vision Transformer (ViT) for the identification of progression of MS on MRI. 170 patients were included. 100 unseen paired images were randomly used for testing. Both U-Net and ViT had 94% accuracy while GPT4V had 85%. GPT4V gave overly cautious non-answers in 6 cases. GPT4V had a precision, recall and F1 score of 0.896, 0.915, 0.905 compared to 1.0, 0.88 and 0.936 for U-Net and 0.94, 0.94, 0.94 for ViT. The impressive performance compared to trained models and a no-code drag and drop interface suggest GPT4V has the potential to disrupt AI radiology research. However misclassified cases, hallucinations and overly cautious non-answers confirm that it is not ready for clinical use. GPT4Vs widespread availability and relatively high error rate highlight the need for caution and education for lay-users, especially those with limited access to expert healthcare. Key pointsO_LIEven without fine tuning and without the need for prior coding experience or additional hardware, GPT4V can perform a zero-shot radiologic change detection task with reasonable accuracy. C_LIO_LIWe find GPT4V does not match the performance of established state of the art computer vision models. GPT4Vs performance metrics are more similar to the vision transformers than the convolutional neural networks, giving some possible insight into its underlying architecture. C_LIO_LIThis is an exploratory experimental study and GPT4V is not intended for use as a medical device. C_LI Summary statementGPT4V can identify radiologic progression of Multiple Sclerosis in a simplified experimental setting. However GPT4V is not a medical device and its widespread availability and relatively high error rate highlight the need for caution and education for lay-users, especially those with limited access to expert healthcare.\nPerformance of Multimodal GPT-4V on USMLE with Image: Potential for Imaging Diagnostic Support with Explanations\nAuthors: Yang, Z.; Yao, Z.; Tasmin, M.; Vashisht, P.; Jang, W. S.; Wang, B.; Ouyang, F.; Berlowitz, D.; Yu, H.\nScore: 2.8, Published: 2023-11-15 DOI: 10.1101/2023.10.26.23297629\nImportanceUsing artificial intelligence (AI) to help clinical diagnoses has been an active research topic for more than six decades. Few research however has the scale and accuracy that can be turned into clinical practice. The tide may be turned today with the power of large language models (LLMs). In this application, we evaluated the accuracy of medical license exam using the newly released Generative Pre-trained Transformer 4 with vision (GPT-4V), a large multimodal model trained to analyze image inputs with the text instructions from the user. This study is the first to evaluate GPTs for interpreting medical images. ObjectiveThis study aimed to evaluate the performance of GPT-4V on medical licensing examination questions with images, as well as to analyze interpretability. Design, Setting, and ParticipantsWe used 3 sets of multiple-choice questions with images to evaluate GPT-4Vs performance. The first set was the United States Medical Licensing Examination (USMLE) from the National Board of Medical Examiners (NBME) sample questions in step1, step2CK, and step3. The second set was derived from AMBOSS, a commonly used question bank for medical students, which also provides statistics on question difficulty and the performance on an exam relative to the user base. The third set was the Diagnostic Radiology Qualifying Core Exam (DRQCE) from the American Board of Radiology. The study (including data analysis) was conducted from September to October 2023. Main Outcomes and MeasuresThe choice accuracy of GPT-4V was compared to two other large language models, GPT-4 and ChatGPT. The GPT-4V explanation was evaluated across 4 qualitative metrics: image misunderstanding, text hallucination, reasoning error, and non-medical error. ResultsOf the 3 exams with images, NBME, AMBOSS, and DRQCE, GPT-4V achieved accuracies of 86.2%, 62.0%, and 73.1%, respectively. GPT-4V outperformed ChatGPT and GPT-4 by 131.8% and 64.5% on average across various data sets. The model demonstrated a decreasing trend in performance as question difficulty increased in the AMBOSS dataset. GPT-4V achieves an accuracy of 90.7% in the full USMLE exam, outperforming the passing threshold of about 60% accuracy. Among the incorrect answers, 75.9% of responses included misinterpretation of the image. However, 39.0% of them could be easily solved with a short hint. ConclusionIn this cross-sectional study, GPT-4V achieved a high accuracy of USMLE that was in the 70th - 80th percentile with AMBOSS users preparing for the exam. The results suggest the potential of GPT-4V for clinical decision support. However, GPT-4V generated explanation revealed several issues. It needs to improve explanation quality for potential use in clinical decision support.\n",
  "wordCount" : "822",
  "inLanguage": "en",
  "datePublished": "2023-11-29T10:40:14Z",
  "dateModified": "2023-11-29T10:40:14Z",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://trxiv.yorks0n.com/posts/radiology-and-imaging/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "TRxiv2",
    "logo": {
      "@type": "ImageObject",
      "url": "https://trxiv.yorks0n.com/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://trxiv.yorks0n.com" accesskey="h" title="TRxiv2 (Alt + H)">TRxiv2</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
        </ul>
    </nav>
</header>
<main class="main">
<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title">
      radiology and imaging
    </h1>
    <div class="post-meta">&lt;span&gt;updated on November 29, 2023&lt;/span&gt;

</div>
  </header> 
  <div class="post-content"><div class="accordion accordion-flush" id="accordionFlushExample"><div class="accordion-item">
    <h3 class="accordion-header" id="flush-heading10.1101/2023.11.15.23298499">
      <button class="accordion-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapse10.1101/2023.11.15.23298499" aria-expanded="false" aria-controls="flush-collapse10.1101/2023.11.15.23298499">
        <p class="paperTitle">Spot the Difference: Can ChatGPT4-Vision Transform Radiology Artificial Intelligence?</p>
      </button>
    </h3>
    <div id="flush-collapse10.1101/2023.11.15.23298499" class="accordion-collapse collapse" aria-labelledby="flush-heading10.1101/2023.11.15.23298499" data-bs-parent="#accordionFlushExample">
      <div class="accordion-body">
        <p class="author">Authors: Kelly, B. S. S.; Duignan, S.; Mathur, P.; Dillon, H.; Lee, E.; Yeom, K. W.; Keane, P.; Killeen, R. P.; Lawlor, A.</p>
        <p class="info">Score: 3.8, Published: 2023-11-18 </p>
        <p class="info">DOI: <a href='https://doi.org/10.1101/2023.11.15.23298499' target='https://doi.org/10.1101/2023.11.15.23298499'> 10.1101/2023.11.15.23298499</a></p>
        <p class="abstract">OpenAIs flagship Large Language Model ChatGPT can now accept image input (GPT4V). &#34;Spot the Difference&#34; and &#34;Medical&#34; have been suggested as emerging applications. The interpretation of medical images is a dynamic process not a static task. Diagnosis and treatment of Multiple Sclerosis is dependent on identification of radiologic change. We aimed to compare the zero-shot performance of GPT4V to a trained U-Net and Vision Transformer (ViT) for the identification of progression of MS on MRI.

170 patients were included. 100 unseen paired images were randomly used for testing. Both U-Net and ViT had 94% accuracy while GPT4V had 85%. GPT4V gave overly cautious non-answers in 6 cases. GPT4V had a precision, recall and F1 score of 0.896, 0.915, 0.905 compared to 1.0, 0.88 and 0.936 for U-Net and 0.94, 0.94, 0.94 for ViT.

The impressive performance compared to trained models and a no-code drag and drop interface suggest GPT4V has the potential to disrupt AI radiology research. However misclassified cases, hallucinations and overly cautious non-answers confirm that it is not ready for clinical use. GPT4Vs widespread availability and relatively high error rate highlight the need for caution and education for lay-users, especially those with limited access to expert healthcare.

Key pointsO_LIEven without fine tuning and without the need for prior coding experience or additional hardware, GPT4V can perform a zero-shot radiologic change detection task with reasonable accuracy.
C_LIO_LIWe find GPT4V does not match the performance of established state of the art computer vision models. GPT4Vs performance metrics are more similar to the vision transformers than the convolutional neural networks, giving some possible insight into its underlying architecture.
C_LIO_LIThis is an exploratory experimental study and GPT4V is not intended for use as a medical device.
C_LI

Summary statementGPT4V can identify radiologic progression of Multiple Sclerosis in a simplified experimental setting. However GPT4V is not a medical device and its widespread availability and relatively high error rate highlight the need for caution and education for lay-users, especially those with limited access to expert healthcare.</p>
      </div>
    </div>
  </div><div class="accordion-item">
    <h3 class="accordion-header" id="flush-heading10.1101/2023.10.26.23297629">
      <button class="accordion-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapse10.1101/2023.10.26.23297629" aria-expanded="false" aria-controls="flush-collapse10.1101/2023.10.26.23297629">
        <p class="paperTitle">Performance of Multimodal GPT-4V on USMLE with Image: Potential for Imaging Diagnostic Support with Explanations</p>
      </button>
    </h3>
    <div id="flush-collapse10.1101/2023.10.26.23297629" class="accordion-collapse collapse" aria-labelledby="flush-heading10.1101/2023.10.26.23297629" data-bs-parent="#accordionFlushExample">
      <div class="accordion-body">
        <p class="author">Authors: Yang, Z.; Yao, Z.; Tasmin, M.; Vashisht, P.; Jang, W. S.; Wang, B.; Ouyang, F.; Berlowitz, D.; Yu, H.</p>
        <p class="info">Score: 2.8, Published: 2023-11-15 </p>
        <p class="info">DOI: <a href='https://doi.org/10.1101/2023.10.26.23297629' target='https://doi.org/10.1101/2023.10.26.23297629'> 10.1101/2023.10.26.23297629</a></p>
        <p class="abstract">ImportanceUsing artificial intelligence (AI) to help clinical diagnoses has been an active research topic for more than six decades. Few research however has the scale and accuracy that can be turned into clinical practice. The tide may be turned today with the power of large language models (LLMs). In this application, we evaluated the accuracy of medical license exam using the newly released Generative Pre-trained Transformer 4 with vision (GPT-4V), a large multimodal model trained to analyze image inputs with the text instructions from the user. This study is the first to evaluate GPTs for interpreting medical images.

ObjectiveThis study aimed to evaluate the performance of GPT-4V on medical licensing examination questions with images, as well as to analyze interpretability.

Design, Setting, and ParticipantsWe used 3 sets of multiple-choice questions with images to evaluate GPT-4Vs performance. The first set was the United States Medical Licensing Examination (USMLE) from the National Board of Medical Examiners (NBME) sample questions in step1, step2CK, and step3. The second set was derived from AMBOSS, a commonly used question bank for medical students, which also provides statistics on question difficulty and the performance on an exam relative to the user base. The third set was the Diagnostic Radiology Qualifying Core Exam (DRQCE) from the American Board of Radiology. The study (including data analysis) was conducted from September to October 2023.

Main Outcomes and MeasuresThe choice accuracy of GPT-4V was compared to two other large language models, GPT-4 and ChatGPT. The GPT-4V explanation was evaluated across 4 qualitative metrics: image misunderstanding, text hallucination, reasoning error, and non-medical error.

ResultsOf the 3 exams with images, NBME, AMBOSS, and DRQCE, GPT-4V achieved accuracies of 86.2%, 62.0%, and 73.1%, respectively. GPT-4V outperformed ChatGPT and GPT-4 by 131.8% and 64.5% on average across various data sets. The model demonstrated a decreasing trend in performance as question difficulty increased in the AMBOSS dataset. GPT-4V achieves an accuracy of 90.7% in the full USMLE exam, outperforming the passing threshold of about 60% accuracy. Among the incorrect answers, 75.9% of responses included misinterpretation of the image. However, 39.0% of them could be easily solved with a short hint.

ConclusionIn this cross-sectional study, GPT-4V achieved a high accuracy of USMLE that was in the 70th - 80th percentile with AMBOSS users preparing for the exam. The results suggest the potential of GPT-4V for clinical decision support. However, GPT-4V generated explanation revealed several issues. It needs to improve explanation quality for potential use in clinical decision support.</p>
      </div>
    </div>
  </div>
</div>









<script src="/js/bundle.min.js" defer></script>



  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2023 <a href="https://trxiv.yorks0n.com">TRxiv2</a></span>
    <span>
        Â· Made by Yorkson
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
