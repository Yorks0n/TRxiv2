<!DOCTYPE html>
<html lang="en" dir="auto">

<head>
<head>
    <title>neuroscience</title>
    <meta charset="utf-8">
    <meta name="description"
        content="Website meta description for google search results go here" />
    <meta name="dc.relation" content="https://trxiv.yorks0n.com" />
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="theme-color" content="#1A94D2" />

    

    
    
    
    <link rel="stylesheet" href="/css/main.min.css" media="screen">

</head>


<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>neuroscience | TRxiv2</title>
<meta name="keywords" content="">
<meta name="description" content="Oscillations in an Artificial Neural Network Convert Competing Inputs into a Temporal Code
Authors: Duecker, K.; Idiart, M.; van Gerven, M. A.; Jensen, O.
Score: 29.4, Published: 2023-11-27 DOI: 10.1101/2023.11.27.568876
Deep convolutional neural networks (CNNs) resemble the hierarchically organised neural representations in the primate visual ventral stream. However, these models typically disregard the temporal dynamics experimentally observed in these areas. For instance, alpha oscillations dominate the dynamics of the human visual cortex, yet the computational relevance of oscillations is rarely considered in artificial neural networks (ANNs).">
<meta name="author" content="">
<link rel="canonical" href="https://trxiv.yorks0n.com/posts/neuroscience/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.904bd1e751cdd2a584fa6bed3fa1166dfd8ec9949ebfd0c4d69c5add5e17c23d.css" integrity="sha256-kEvR51HN0qWE&#43;mvtP6EWbf2OyZSev9DE1pxa3V4Xwj0=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js" integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG&#43;9vmJ0cTS&#43;ovo0FeA="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://trxiv.yorks0n.com/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://trxiv.yorks0n.com/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://trxiv.yorks0n.com/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://trxiv.yorks0n.com/apple-touch-icon.png">
<link rel="mask-icon" href="https://trxiv.yorks0n.com/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:title" content="neuroscience" />
<meta property="og:description" content="Oscillations in an Artificial Neural Network Convert Competing Inputs into a Temporal Code
Authors: Duecker, K.; Idiart, M.; van Gerven, M. A.; Jensen, O.
Score: 29.4, Published: 2023-11-27 DOI: 10.1101/2023.11.27.568876
Deep convolutional neural networks (CNNs) resemble the hierarchically organised neural representations in the primate visual ventral stream. However, these models typically disregard the temporal dynamics experimentally observed in these areas. For instance, alpha oscillations dominate the dynamics of the human visual cortex, yet the computational relevance of oscillations is rarely considered in artificial neural networks (ANNs)." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://trxiv.yorks0n.com/posts/neuroscience/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-12-03T10:38:01+00:00" />
<meta property="article:modified_time" content="2023-12-03T10:38:01+00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="neuroscience"/>
<meta name="twitter:description" content="Oscillations in an Artificial Neural Network Convert Competing Inputs into a Temporal Code
Authors: Duecker, K.; Idiart, M.; van Gerven, M. A.; Jensen, O.
Score: 29.4, Published: 2023-11-27 DOI: 10.1101/2023.11.27.568876
Deep convolutional neural networks (CNNs) resemble the hierarchically organised neural representations in the primate visual ventral stream. However, these models typically disregard the temporal dynamics experimentally observed in these areas. For instance, alpha oscillations dominate the dynamics of the human visual cortex, yet the computational relevance of oscillations is rarely considered in artificial neural networks (ANNs)."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Posts",
      "item": "https://trxiv.yorks0n.com/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  3 ,
      "name": "neuroscience",
      "item": "https://trxiv.yorks0n.com/posts/neuroscience/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "neuroscience",
  "name": "neuroscience",
  "description": "Oscillations in an Artificial Neural Network Convert Competing Inputs into a Temporal Code\nAuthors: Duecker, K.; Idiart, M.; van Gerven, M. A.; Jensen, O.\nScore: 29.4, Published: 2023-11-27 DOI: 10.1101/2023.11.27.568876\nDeep convolutional neural networks (CNNs) resemble the hierarchically organised neural representations in the primate visual ventral stream. However, these models typically disregard the temporal dynamics experimentally observed in these areas. For instance, alpha oscillations dominate the dynamics of the human visual cortex, yet the computational relevance of oscillations is rarely considered in artificial neural networks (ANNs).",
  "keywords": [
    
  ],
  "articleBody": " Oscillations in an Artificial Neural Network Convert Competing Inputs into a Temporal Code\nAuthors: Duecker, K.; Idiart, M.; van Gerven, M. A.; Jensen, O.\nScore: 29.4, Published: 2023-11-27 DOI: 10.1101/2023.11.27.568876\nDeep convolutional neural networks (CNNs) resemble the hierarchically organised neural representations in the primate visual ventral stream. However, these models typically disregard the temporal dynamics experimentally observed in these areas. For instance, alpha oscillations dominate the dynamics of the human visual cortex, yet the computational relevance of oscillations is rarely considered in artificial neural networks (ANNs). We propose an ANN that embraces oscillatory dynamics with the computational purpose of converting simultaneous inputs, presented at two different locations, into a temporal code. The network was trained to classify three individually presented letters. Post-training, we added semi-realistic temporal dynamics to the hidden layer, introducing relaxation dynamics in the hidden units as well as pulsed inhibition mimicking neuronal alpha oscillations. Without these dynamics, the trained network correctly classified individual letters but produced a mixed output when presented with two letters simultaneously, elucidating a bottleneck problem. When introducing refraction and oscillatory inhibition, the output nodes corresponding to the two stimuli activated sequentially, ordered along the phase of the inhibitory oscillations. Our model provides a novel approach for implementing multiplexing in ANNs. It further produces experimentally testable predictions of how the primate visual system handles competing stimuli.\nFast and light efficient remote focusing for volumetric voltage imaging\nAuthors: BoÌˆhm, U. L.; Judkewitz, B.\nScore: 28.2, Published: 2023-11-28 DOI: 10.1101/2023.11.28.568783\nVoltage imaging holds great potential for biomedical research by enabling noninvasive recording of the electrical activity of excitable cells such as neurons or cardiomyocytes. Camera-based detection can record from hundreds of cells in parallel, but imaging entire volumes is limited by the need to focus through the sample at high speeds. Remote focusing techniques can remedy this drawback, but have so far been either too slow or light inefficient. Here, we introduce FLIPR, a new approach for remote focusing that doubles the light efficiency and enables high-speed volumetric voltage imaging at 500 volumes/s. We show the potential of our approach by combining it with lightsheet imaging in the zebrafish spinal cord to record from \u003e100 spontaneously active neurons in parallel.\nOptical constraints on two-photon voltage imaging\nAuthors: Davis, H. C.; Brooks, F. P.; Wong-Campos, J. D.; Cohen, A. E.\nScore: 28.8, Published: 2023-11-18 DOI: 10.1101/2023.11.18.567441\nGenetically encoded voltage indicators (GEVIs) are a valuable tool for studying neural circuits in vivo, but the relative merits of one-photon (1P) vs. two-photon (2P) voltage imaging are not well characterized. Here we compare the photophysical and imaging properties of commonly used GEVIs under 1P and 2P excitation. 2P excitation requires [~]104-fold more illumination power per cell to produce comparable photon count rates to 1P excitation, driving a stringent tradeoff between shot noise and tissue photodamage.\nPropofol anesthesia destabilizes neural dynamics across cortex\nAuthors: Eisen, A. J.; Kozachkov, L.; Bastos, A. M.; Donoghue, J. A.; Mahnke, M. K.; Brincat, S. L.; Chandra, S.; Brown, E. N.; Fiete, I.; Miller, E. K.\nScore: 27.8, Published: 2023-11-25 DOI: 10.1101/2023.11.24.568595\nEvery day, hundreds of thousands of people undergo general anesthesia. One hypothesis is that anesthesia disrupts dynamic stability, the ability of the brain to balance excitability with the need to be stable and thus controllable. We tested this hypothesis using a new method for quantifying population-level dynamic stability in complex systems, Delayed Linear Analysis for Stability Estimation (DeLASE). Propofol was used to transition animals between the awake state and anesthetized unconsciousness. DeLASE was applied to macaque cortex local field potentials (LFPs). We found that neural dynamics were more unstable in unconsciousness compared to the awake state. Cortical trajectories mirrored predictions from destabilized linear systems. We mimicked the effect of propofol in simulated neural networks by increasing inhibitory tone. Paradoxically, increased inhibition also destabilized the networks. Our results suggest that anesthesia disrupts dynamical stability that is required for consciousness.\nAn arginine-rich nuclear localization signal (ArgiNLS) strategy for streamlined image segmentation of single-cells\nAuthors: Szelenyi, E. R.; Navarrete, J. S.; Murry, A. D.; Zhang, Y.; Girven, K. S.; Kuo, L.; Cline, M. M.; Bernstein, M. X.; Burdyniuk, M.; Bowler, B.; Goodwin, N. L.; Juarez, B.; Zweifel, L.; Golden, S.\nScore: 22.0, Published: 2023-11-23 DOI: 10.1101/2023.11.22.568319\nHigh-throughput volumetric fluorescent microscopy pipelines can spatially integrate whole-brain structure and function at the foundational level of single-cells. However, conventional fluorescent protein (FP) modifications used to discriminate single-cells possess limited efficacy or are detrimental to cellular health. Here, we introduce a synthetic and non-deleterious nuclear localization signal (NLS) tag strategy, called Arginine-rich NLS (ArgiNLS), that optimizes genetic labeling and downstream image segmentation of single-cells by restricting FP localization near-exclusively in the nucleus through a poly-arginine mechanism. A single N-terminal ArgiNLS tag provides modular nuclear restriction consistently across spectrally separate FP variants. ArgiNLS performance in vivo displays functional conservation across major cortical cell classes, and in response to both local and systemic brain wide AAV administration. Crucially, the high signal-to-noise ratio afforded by ArgiNLS enhances ML-automated segmentation of single-cells due to rapid classifier training and enrichment of labeled cell detection within 2D brain sections or 3D volumetric whole-brain image datasets, derived from both staining-amplified and native signal. This genetic strategy provides a simple and flexible basis for precise image segmentation of genetically labeled single-cells at scale and paired with behavioral procedures. Significance StatementQuantifying labeled cells in fluorescent microscopy is a fundamental aspect of a modern biology. Critically, the use of short nuclear localization sequences (NLS) is a key genetic modification for discriminating single-cells labeled with fluorescent proteins (FPs). However, mainstay NLS approaches typically localize proteins to the nucleus with limited efficacy, while alternative non-NLS tag strategies can enhance efficacy at the cost of cellular health. Thus, quantitative cell counting using FP labels remains suboptimal or not compatible with health and behavior. Here, we present a novel genetic tagging strategy - named ArgiNLS - that flexibly and safely achieves FP nuclear restriction across the brain to facilitate machine learning-based segmentation of single-cells at scale, delivering a timely update to the behavioral neuroscientists toolkit.\nDistributed representations of innate behaviors in the hypothalamus do not predict specialized functional centers\nAuthors: Stagkourakis, S.; Spigolon, G.; Marks, M.; Feyder, M.; Kim, J.; Perona, P.; Pachitariu, M.; Anderson, D. J.\nScore: 42.3, Published: 2023-11-21 DOI: 10.1101/2023.11.21.568163\nSurvival behaviors are orchestrated by hardwired circuits located in deep subcortical brain regions, most prominently the hypothalamus. Artificial activation of spatially localized, genetically defined hypothalamic cell populations is known to trigger distinct behaviors, suggesting a nucleus-centered organization of behavioral control. However, no study has investigated the hypothalamic representation of innate behaviors using unbiased, large-scale single neuron recordings. Here, using custom silicon probes, we performed recordings across the rostro-caudal extent of the medial hypothalamus in freely moving animals engaged in a diverse array of social and predator defense (\"fear\") behaviors. Nucleus-averaged activity revealed spatially distributed generic \"ignition signals\" that occurred at the onset of each behavior, and did not identify sparse, nucleus-specific behavioral representations. Single-unit analysis revealed that social and fear behavior classes are encoded by activity in distinct sets of spatially distributed neuronal ensembles spanning the entire hypothalamic rostro-caudal axis. Individual ensemble membership, however, was drawn from neurons in 3-4 adjacent nuclei. Mixed selectivity was identified as the most prevalent mode of behavior representation by individual hypothalamic neurons. Encoding models indicated that a significant fraction of the variance in single neuron activity is explained by behavior. This work reveals that innate behaviors are encoded in the hypothalamus by activity in spatially distributed neural ensembles that each span multiple neighboring nuclei, complementing the prevailing view of hypothalamic behavioral control by single nucleus-restricted cell types derived from perturbational studies.\nVentral hippocampal interneurons govern extinction and relapse of contextual associations\nAuthors: Lacagnina, A. F.; Dong, T. N.; Iyer, R. R.; Khan, S.; Mohamed, M. K.; Clem, R. L.\nScore: 18.1, Published: 2023-11-28 DOI: 10.1101/2023.11.28.568835\nABSTRACT/SUMMARYContextual associations are critical for survival but must be extinguished when new conditions render them nonproductive. By most accounts, extinction forms a new memory that competes with the original association for control over behavior, but the mechanisms underlying this competition remain largely enigmatic. Here we find the retrieval of contextual fear conditioning and extinction yield contrasting patterns of activity in prefrontal cortex and ventral hippocampus. Within ventral CA1, activation of somatostatin-expressing interneurons (SST-INs) occurs preferentially during extinction retrieval and correlates with differences in input synaptic transmission. Optogenetic manipulation of these cells but not parvalbumin interneurons (PV-INs) elicits bidirectional changes in fear expression following extinction, and the ability of SST-INs to gate fear is specific to the context in which extinction was acquired. A similar pattern of results was obtained following reward-based extinction. These data show that ventral hippocampal SST-INs are critical for extinguishing prior associations and thereby gate relapse of both aversive and appetitive responses.\nPIEZO2-dependent rapid pain system in humans and mice.\nAuthors: Bouchatta, O.; Brodzki, M.; Manouze, H.; Carballo, G. B.; Kindstrom, E.; de-Faria, F. M.; Yu, H.; Kao, A. R.; Thorell, O.; Liljencrantz, J.; Ng, K. K.; Frangos, E.; Ragnemalm, B.; Saade, D.; Bharucha-Goebel, D.; Szczot, I.; Moore, W.; Terejko, K.; Cole, J.; Bonnemann, C.; Luo, W.; Mahns, D. A.; Larsson, M.; Gerling, G. J.; Marshall, A. G.; Chesler, A. T.; Olausson, H.; Nagi, S. S.; Szczot, M.\nScore: 17.9, Published: 2023-12-02 DOI: 10.1101/2023.12.01.569650\nThe PIEZO2 ion channel is critical for transducing light touch into neural signals but is not considered necessary for transducing acute pain in humans. Here, we discovered an exception - a form of mechanical pain evoked by hair pulling. Based on observations in a rare group of individuals with PIEZO2 deficiency syndrome, we demonstrated that hair-pull pain is dependent on PIEZO2 transduction. Studies in control participants showed that hair-pull pain triggered a distinct nocifensive response, including a nociceptive reflex. Observations in rare A{beta} deafferented individuals and nerve conduction block studies in control participants revealed that hair-pull pain perception is dependent on A{beta} input. Single-unit axonal recordings revealed that a class of cooling-responsive myelinated nociceptors in human skin is selectively tuned to painful hair-pull stimuli. Further, we pharmacologically mapped these nociceptors to a specific transcriptomic class. Finally, using functional imaging in mice, we demonstrated that in a homologous nociceptor, Piezo2 is necessary for high-sensitivity, robust activation by hair-pull stimuli. Together, we have demonstrated that hair-pulling evokes a distinct type of pain with conserved behavioral, neural, and molecular features across humans and mice.\nExpansion of the neocortex and protection from neurodegeneration by in vivo transient reprogramming\nAuthors: Shen, Y.-R.; Zaballa, S.; Bech, X.; Sancho-Balsells, A.; Diaz-Cifuentes, C.; Seyit-Bremer, G.; Ballasch, I.; Alcazar, N.; Alberch, J.; Abad, M.; Serrano, M.; Klein, R.; Giralt, A.; del Toro, D.\nScore: 16.8, Published: 2023-11-27 DOI: 10.1101/2023.11.27.568858\nYamanaka factors (YFs) can reverse some aging features in mammalian tissues, but their effects on the brain remain largely unexplored. Here, we induced YFs in the mouse brain in a controlled spatio-temporal manner in two different scenarios: brain development, and adult stages in the context of neurodegeneration. Embryonic induction of YFs perturbed cell identity of both progenitors and neurons, but transient and low-level expression is tolerated by these cells during development. Under these conditions, YFs induction led to expanded neurogenesis, increased number of upper cortical neurons, and enhanced motor and social behavior of adult mice. Additionally, controlled YF induction is tolerated by principal neurons in the adult dorsal hippocampus and prevented the development of several hallmarks of Alzheimers disease, including cognitive decline and altered molecular signatures, in the 5xFAD mouse model. Overall, these results highlight the powerful impact of YFs on neurogenesis and their potential use in brain disorders. HighlightsO_LITransient Yamanaka factor (YF) expression during development expands neocortex C_LIO_LIYF-treated mice show enhanced cognitive skills C_LIO_LIIntermitent YF expression is tolerated by adult principal hippocampal neurons C_LIO_LILong-term intermitent YF reprogramming is protective in an AD mouse model C_LI\nUnravelling the neural dynamics of hypnotic susceptibility: Aperiodic neural activity as a central feature of hypnosis\nAuthors: Landry, M.; da Silva Castanheira, J.; Sackur, J.; Raz, A.; Ogez, D.; Rainville, P.; Jerbi, K.\nScore: 35.0, Published: 2023-11-17 DOI: 10.1101/2023.11.16.567097\nThe ability for hypnotic responding is marked by great inter-individual variation in the population, while the neural underpinning of this variability remains elusive. The current work leveraged multivariate statistics and machine learning to probe the neural dynamics underlying inter-individual differences in hypnotic susceptibility. We assessed the efficacy of linear classifiers in distinguishing between high and low hypnotic susceptible individuals using neural features from resting-state electroencephalography (EEG) both pre- and post-hypnotic induction. Our focus encompassed both aperiodic and periodic components of the power spectrum, and graph theoretical measures derived from functional connectivity patterns. Several neural features from both pre- and post-induction significantly differentiated susceptibility levels, which underscores the complex dynamics of hypnotic phenomena. Based on model comparisons and feature ranking, we discerned the pre-induction aperiodic exponent as the primary discriminating neural feature, while periodic activity did not differ between groups. This novel finding not only resonates with the increasing emphasis on this neural component in broader EEG research but also promotes the idea that the primary neural distinction in hypnotic susceptibility is evident at baseline, even before hypnosis. Based on prevailing interpretation of aperiodicity in the EEG signal, our findings support the idea that hypnotic susceptibility might be an inherent trait reflected in the balance of cortical excitation and inhibition. Significance StatementHypnotic phenomena reflect the ability to alter ones subjective experiences based on targeted verbal suggestions. While research has made strides in understanding the cognitive aspects underlying the variability in ones susceptibility to hypnosis, the brain correlates remain largely elusive. Addressing this gap, our study employs multivariate pattern classification and machine learning to predict hypnotic susceptibility. By recording electroencephalography (EEG) before and after hypnotic induction and analyzing diverse neurophysiological features, we identify several features that differentiate between high and low hypnotic susceptible individuals for both pre- and post-induction periods, which underscores the multifaceted nature of hypnotic phenomena. Our analysis revealed that the paramount discriminative feature is arrhythmic (i.e., non-oscillatory) EEG activity prior to the induction--a novel discovery in the field. This finding, that the chief EEG marker is observed even before hypnosis, aligns with the idea that hypnotic susceptibility represents an inherent trait reflecting the balance between excitation and inhibition in neural activity.\n",
  "wordCount" : "2353",
  "inLanguage": "en",
  "datePublished": "2023-12-03T10:38:01Z",
  "dateModified": "2023-12-03T10:38:01Z",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://trxiv.yorks0n.com/posts/neuroscience/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "TRxiv2",
    "logo": {
      "@type": "ImageObject",
      "url": "https://trxiv.yorks0n.com/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://trxiv.yorks0n.com" accesskey="h" title="TRxiv2 (Alt + H)">TRxiv2</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
        </ul>
    </nav>
</header>
<main class="main">
<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title">
      neuroscience
    </h1>
    <div class="post-meta">&lt;span&gt;updated on December 3, 2023&lt;/span&gt;

</div>
  </header> 
  <div class="post-content"><div class="accordion accordion-flush" id="accordionFlushExample"><div class="accordion-item">
    <h3 class="accordion-header" id="flush-heading10.1101/2023.11.27.568876">
      <button class="accordion-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapse10.1101/2023.11.27.568876" aria-expanded="false" aria-controls="flush-collapse10.1101/2023.11.27.568876">
        <p class="paperTitle">Oscillations in an Artificial Neural Network Convert Competing Inputs into a Temporal Code</p>
      </button>
    </h3>
    <div id="flush-collapse10.1101/2023.11.27.568876" class="accordion-collapse collapse" aria-labelledby="flush-heading10.1101/2023.11.27.568876" data-bs-parent="#accordionFlushExample">
      <div class="accordion-body">
        <p class="author">Authors: Duecker, K.; Idiart, M.; van Gerven, M. A.; Jensen, O.</p>
        <p class="info">Score: 29.4, Published: 2023-11-27 </p>
        <p class="info">DOI: <a href='https://doi.org/10.1101/2023.11.27.568876' target='https://doi.org/10.1101/2023.11.27.568876'> 10.1101/2023.11.27.568876</a></p>
        <p class="abstract">Deep convolutional neural networks (CNNs) resemble the hierarchically organised neural representations in the primate visual ventral stream. However, these models typically disregard the temporal dynamics experimentally observed in these areas. For instance, alpha oscillations dominate the dynamics of the human visual cortex, yet the computational relevance of oscillations is rarely considered in artificial neural networks (ANNs). We propose an ANN that embraces oscillatory dynamics with the computational purpose of converting simultaneous inputs, presented at two different locations, into a temporal code. The network was trained to classify three individually presented letters. Post-training, we added semi-realistic temporal dynamics to the hidden layer, introducing relaxation dynamics in the hidden units as well as pulsed inhibition mimicking neuronal alpha oscillations. Without these dynamics, the trained network correctly classified individual letters but produced a mixed output when presented with two letters simultaneously, elucidating a bottleneck problem. When introducing refraction and oscillatory inhibition, the output nodes corresponding to the two stimuli activated sequentially, ordered along the phase of the inhibitory oscillations. Our model provides a novel approach for implementing multiplexing in ANNs. It further produces experimentally testable predictions of how the primate visual system handles competing stimuli.</p>
      </div>
    </div>
  </div><div class="accordion-item">
    <h3 class="accordion-header" id="flush-heading10.1101/2023.11.28.568783">
      <button class="accordion-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapse10.1101/2023.11.28.568783" aria-expanded="false" aria-controls="flush-collapse10.1101/2023.11.28.568783">
        <p class="paperTitle">Fast and light efficient remote focusing for volumetric voltage imaging</p>
      </button>
    </h3>
    <div id="flush-collapse10.1101/2023.11.28.568783" class="accordion-collapse collapse" aria-labelledby="flush-heading10.1101/2023.11.28.568783" data-bs-parent="#accordionFlushExample">
      <div class="accordion-body">
        <p class="author">Authors: BoÌˆhm, U. L.; Judkewitz, B.</p>
        <p class="info">Score: 28.2, Published: 2023-11-28 </p>
        <p class="info">DOI: <a href='https://doi.org/10.1101/2023.11.28.568783' target='https://doi.org/10.1101/2023.11.28.568783'> 10.1101/2023.11.28.568783</a></p>
        <p class="abstract">Voltage imaging holds great potential for biomedical research by enabling noninvasive recording of the electrical activity of excitable cells such as neurons or cardiomyocytes. Camera-based detection can record from hundreds of cells in parallel, but imaging entire volumes is limited by the need to focus through the sample at high speeds. Remote focusing techniques can remedy this drawback, but have so far been either too slow or light inefficient. Here, we introduce FLIPR, a new approach for remote focusing that doubles the light efficiency and enables high-speed volumetric voltage imaging at 500 volumes/s. We show the potential of our approach by combining it with lightsheet imaging in the zebrafish spinal cord to record from &gt;100 spontaneously active neurons in parallel.</p>
      </div>
    </div>
  </div><div class="accordion-item">
    <h3 class="accordion-header" id="flush-heading10.1101/2023.11.18.567441">
      <button class="accordion-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapse10.1101/2023.11.18.567441" aria-expanded="false" aria-controls="flush-collapse10.1101/2023.11.18.567441">
        <p class="paperTitle">Optical constraints on two-photon voltage imaging</p>
      </button>
    </h3>
    <div id="flush-collapse10.1101/2023.11.18.567441" class="accordion-collapse collapse" aria-labelledby="flush-heading10.1101/2023.11.18.567441" data-bs-parent="#accordionFlushExample">
      <div class="accordion-body">
        <p class="author">Authors: Davis, H. C.; Brooks, F. P.; Wong-Campos, J. D.; Cohen, A. E.</p>
        <p class="info">Score: 28.8, Published: 2023-11-18 </p>
        <p class="info">DOI: <a href='https://doi.org/10.1101/2023.11.18.567441' target='https://doi.org/10.1101/2023.11.18.567441'> 10.1101/2023.11.18.567441</a></p>
        <p class="abstract">Genetically encoded voltage indicators (GEVIs) are a valuable tool for studying neural circuits in vivo, but the relative merits of one-photon (1P) vs. two-photon (2P) voltage imaging are not well characterized. Here we compare the photophysical and imaging properties of commonly used GEVIs under 1P and 2P excitation. 2P excitation requires [~]104-fold more illumination power per cell to produce comparable photon count rates to 1P excitation, driving a stringent tradeoff between shot noise and tissue photodamage.</p>
      </div>
    </div>
  </div><div class="accordion-item">
    <h3 class="accordion-header" id="flush-heading10.1101/2023.11.24.568595">
      <button class="accordion-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapse10.1101/2023.11.24.568595" aria-expanded="false" aria-controls="flush-collapse10.1101/2023.11.24.568595">
        <p class="paperTitle">Propofol anesthesia destabilizes neural dynamics across cortex</p>
      </button>
    </h3>
    <div id="flush-collapse10.1101/2023.11.24.568595" class="accordion-collapse collapse" aria-labelledby="flush-heading10.1101/2023.11.24.568595" data-bs-parent="#accordionFlushExample">
      <div class="accordion-body">
        <p class="author">Authors: Eisen, A. J.; Kozachkov, L.; Bastos, A. M.; Donoghue, J. A.; Mahnke, M. K.; Brincat, S. L.; Chandra, S.; Brown, E. N.; Fiete, I.; Miller, E. K.</p>
        <p class="info">Score: 27.8, Published: 2023-11-25 </p>
        <p class="info">DOI: <a href='https://doi.org/10.1101/2023.11.24.568595' target='https://doi.org/10.1101/2023.11.24.568595'> 10.1101/2023.11.24.568595</a></p>
        <p class="abstract">Every day, hundreds of thousands of people undergo general anesthesia. One hypothesis is that anesthesia disrupts dynamic stability, the ability of the brain to balance excitability with the need to be stable and thus controllable. We tested this hypothesis using a new method for quantifying population-level dynamic stability in complex systems, Delayed Linear Analysis for Stability Estimation (DeLASE). Propofol was used to transition animals between the awake state and anesthetized unconsciousness. DeLASE was applied to macaque cortex local field potentials (LFPs). We found that neural dynamics were more unstable in unconsciousness compared to the awake state. Cortical trajectories mirrored predictions from destabilized linear systems. We mimicked the effect of propofol in simulated neural networks by increasing inhibitory tone. Paradoxically, increased inhibition also destabilized the networks. Our results suggest that anesthesia disrupts dynamical stability that is required for consciousness.</p>
      </div>
    </div>
  </div><div class="accordion-item">
    <h3 class="accordion-header" id="flush-heading10.1101/2023.11.22.568319">
      <button class="accordion-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapse10.1101/2023.11.22.568319" aria-expanded="false" aria-controls="flush-collapse10.1101/2023.11.22.568319">
        <p class="paperTitle">An arginine-rich nuclear localization signal (ArgiNLS) strategy for streamlined image segmentation of single-cells</p>
      </button>
    </h3>
    <div id="flush-collapse10.1101/2023.11.22.568319" class="accordion-collapse collapse" aria-labelledby="flush-heading10.1101/2023.11.22.568319" data-bs-parent="#accordionFlushExample">
      <div class="accordion-body">
        <p class="author">Authors: Szelenyi, E. R.; Navarrete, J. S.; Murry, A. D.; Zhang, Y.; Girven, K. S.; Kuo, L.; Cline, M. M.; Bernstein, M. X.; Burdyniuk, M.; Bowler, B.; Goodwin, N. L.; Juarez, B.; Zweifel, L.; Golden, S.</p>
        <p class="info">Score: 22.0, Published: 2023-11-23 </p>
        <p class="info">DOI: <a href='https://doi.org/10.1101/2023.11.22.568319' target='https://doi.org/10.1101/2023.11.22.568319'> 10.1101/2023.11.22.568319</a></p>
        <p class="abstract">High-throughput volumetric fluorescent microscopy pipelines can spatially integrate whole-brain structure and function at the foundational level of single-cells. However, conventional fluorescent protein (FP) modifications used to discriminate single-cells possess limited efficacy or are detrimental to cellular health. Here, we introduce a synthetic and non-deleterious nuclear localization signal (NLS) tag strategy, called  Arginine-rich NLS (ArgiNLS), that optimizes genetic labeling and downstream image segmentation of single-cells by restricting FP localization near-exclusively in the nucleus through a poly-arginine mechanism. A single N-terminal ArgiNLS tag provides modular nuclear restriction consistently across spectrally separate FP variants. ArgiNLS performance in vivo displays functional conservation across major cortical cell classes, and in response to both local and systemic brain wide AAV administration. Crucially, the high signal-to-noise ratio afforded by ArgiNLS enhances ML-automated segmentation of single-cells due to rapid classifier training and enrichment of labeled cell detection within 2D brain sections or 3D volumetric whole-brain image datasets, derived from both staining-amplified and native signal. This genetic strategy provides a simple and flexible basis for precise image segmentation of genetically labeled single-cells at scale and paired with behavioral procedures.

Significance StatementQuantifying labeled cells in fluorescent microscopy is a fundamental aspect of a modern biology. Critically, the use of short nuclear localization sequences (NLS) is a key genetic modification for discriminating single-cells labeled with fluorescent proteins (FPs). However, mainstay NLS approaches typically localize proteins to the nucleus with limited efficacy, while alternative non-NLS tag strategies can enhance efficacy at the cost of cellular health. Thus, quantitative cell counting using FP labels remains suboptimal or not compatible with health and behavior. Here, we present a novel genetic tagging strategy - named ArgiNLS - that flexibly and safely achieves FP nuclear restriction across the brain to facilitate machine learning-based segmentation of single-cells at scale, delivering a timely update to the behavioral neuroscientists toolkit.</p>
      </div>
    </div>
  </div><div class="accordion-item">
    <h3 class="accordion-header" id="flush-heading10.1101/2023.11.21.568163">
      <button class="accordion-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapse10.1101/2023.11.21.568163" aria-expanded="false" aria-controls="flush-collapse10.1101/2023.11.21.568163">
        <p class="paperTitle">Distributed representations of innate behaviors in the hypothalamus do not predict specialized functional centers</p>
      </button>
    </h3>
    <div id="flush-collapse10.1101/2023.11.21.568163" class="accordion-collapse collapse" aria-labelledby="flush-heading10.1101/2023.11.21.568163" data-bs-parent="#accordionFlushExample">
      <div class="accordion-body">
        <p class="author">Authors: Stagkourakis, S.; Spigolon, G.; Marks, M.; Feyder, M.; Kim, J.; Perona, P.; Pachitariu, M.; Anderson, D. J.</p>
        <p class="info">Score: 42.3, Published: 2023-11-21 </p>
        <p class="info">DOI: <a href='https://doi.org/10.1101/2023.11.21.568163' target='https://doi.org/10.1101/2023.11.21.568163'> 10.1101/2023.11.21.568163</a></p>
        <p class="abstract">Survival behaviors are orchestrated by hardwired circuits located in deep subcortical brain regions, most prominently the hypothalamus. Artificial activation of spatially localized, genetically defined hypothalamic cell populations is known to trigger distinct behaviors, suggesting a nucleus-centered organization of behavioral control. However, no study has investigated the hypothalamic representation of innate behaviors using unbiased, large-scale single neuron recordings. Here, using custom silicon probes, we performed recordings across the rostro-caudal extent of the medial hypothalamus in freely moving animals engaged in a diverse array of social and predator defense (&#34;fear&#34;) behaviors. Nucleus-averaged activity revealed spatially distributed generic &#34;ignition signals&#34; that occurred at the onset of each behavior, and did not identify sparse, nucleus-specific behavioral representations. Single-unit analysis revealed that social and fear behavior classes are encoded by activity in distinct sets of spatially distributed neuronal ensembles spanning the entire hypothalamic rostro-caudal axis. Individual ensemble membership, however, was drawn from neurons in 3-4 adjacent nuclei. Mixed selectivity was identified as the most prevalent mode of behavior representation by individual hypothalamic neurons. Encoding models indicated that a significant fraction of the variance in single neuron activity is explained by behavior. This work reveals that innate behaviors are encoded in the hypothalamus by activity in spatially distributed neural ensembles that each span multiple neighboring nuclei, complementing the prevailing view of hypothalamic behavioral control by single nucleus-restricted cell types derived from perturbational studies.</p>
      </div>
    </div>
  </div><div class="accordion-item">
    <h3 class="accordion-header" id="flush-heading10.1101/2023.11.28.568835">
      <button class="accordion-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapse10.1101/2023.11.28.568835" aria-expanded="false" aria-controls="flush-collapse10.1101/2023.11.28.568835">
        <p class="paperTitle">Ventral hippocampal interneurons govern extinction and relapse of contextual associations</p>
      </button>
    </h3>
    <div id="flush-collapse10.1101/2023.11.28.568835" class="accordion-collapse collapse" aria-labelledby="flush-heading10.1101/2023.11.28.568835" data-bs-parent="#accordionFlushExample">
      <div class="accordion-body">
        <p class="author">Authors: Lacagnina, A. F.; Dong, T. N.; Iyer, R. R.; Khan, S.; Mohamed, M. K.; Clem, R. L.</p>
        <p class="info">Score: 18.1, Published: 2023-11-28 </p>
        <p class="info">DOI: <a href='https://doi.org/10.1101/2023.11.28.568835' target='https://doi.org/10.1101/2023.11.28.568835'> 10.1101/2023.11.28.568835</a></p>
        <p class="abstract">ABSTRACT/SUMMARYContextual associations are critical for survival but must be extinguished when new conditions render them nonproductive. By most accounts, extinction forms a new memory that competes with the original association for control over behavior, but the mechanisms underlying this competition remain largely enigmatic. Here we find the retrieval of contextual fear conditioning and extinction yield contrasting patterns of activity in prefrontal cortex and ventral hippocampus. Within ventral CA1, activation of somatostatin-expressing interneurons (SST-INs) occurs preferentially during extinction retrieval and correlates with differences in input synaptic transmission. Optogenetic manipulation of these cells but not parvalbumin interneurons (PV-INs) elicits bidirectional changes in fear expression following extinction, and the ability of SST-INs to gate fear is specific to the context in which extinction was acquired. A similar pattern of results was obtained following reward-based extinction. These data show that ventral hippocampal SST-INs are critical for extinguishing prior associations and thereby gate relapse of both aversive and appetitive responses.</p>
      </div>
    </div>
  </div><div class="accordion-item">
    <h3 class="accordion-header" id="flush-heading10.1101/2023.12.01.569650">
      <button class="accordion-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapse10.1101/2023.12.01.569650" aria-expanded="false" aria-controls="flush-collapse10.1101/2023.12.01.569650">
        <p class="paperTitle">PIEZO2-dependent rapid pain system in humans and mice.</p>
      </button>
    </h3>
    <div id="flush-collapse10.1101/2023.12.01.569650" class="accordion-collapse collapse" aria-labelledby="flush-heading10.1101/2023.12.01.569650" data-bs-parent="#accordionFlushExample">
      <div class="accordion-body">
        <p class="author">Authors: Bouchatta, O.; Brodzki, M.; Manouze, H.; Carballo, G. B.; Kindstrom, E.; de-Faria, F. M.; Yu, H.; Kao, A. R.; Thorell, O.; Liljencrantz, J.; Ng, K. K.; Frangos, E.; Ragnemalm, B.; Saade, D.; Bharucha-Goebel, D.; Szczot, I.; Moore, W.; Terejko, K.; Cole, J.; Bonnemann, C.; Luo, W.; Mahns, D. A.; Larsson, M.; Gerling, G. J.; Marshall, A. G.; Chesler, A. T.; Olausson, H.; Nagi, S. S.; Szczot, M.</p>
        <p class="info">Score: 17.9, Published: 2023-12-02 </p>
        <p class="info">DOI: <a href='https://doi.org/10.1101/2023.12.01.569650' target='https://doi.org/10.1101/2023.12.01.569650'> 10.1101/2023.12.01.569650</a></p>
        <p class="abstract">The PIEZO2 ion channel is critical for transducing light touch into neural signals but is not considered necessary for transducing acute pain in humans. Here, we discovered an exception - a form of mechanical pain evoked by hair pulling. Based on observations in a rare group of individuals with PIEZO2 deficiency syndrome, we demonstrated that hair-pull pain is dependent on PIEZO2 transduction. Studies in control participants showed that hair-pull pain triggered a distinct nocifensive response, including a nociceptive reflex. Observations in rare A{beta} deafferented individuals and nerve conduction block studies in control participants revealed that hair-pull pain perception is dependent on A{beta} input. Single-unit axonal recordings revealed that a class of cooling-responsive myelinated nociceptors in human skin is selectively tuned to painful hair-pull stimuli. Further, we pharmacologically mapped these nociceptors to a specific transcriptomic class. Finally, using functional imaging in mice, we demonstrated that in a homologous nociceptor, Piezo2 is necessary for high-sensitivity, robust activation by hair-pull stimuli. Together, we have demonstrated that hair-pulling evokes a distinct type of pain with conserved behavioral, neural, and molecular features across humans and mice.</p>
      </div>
    </div>
  </div><div class="accordion-item">
    <h3 class="accordion-header" id="flush-heading10.1101/2023.11.27.568858">
      <button class="accordion-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapse10.1101/2023.11.27.568858" aria-expanded="false" aria-controls="flush-collapse10.1101/2023.11.27.568858">
        <p class="paperTitle">Expansion of the neocortex and protection from neurodegeneration by in vivo transient reprogramming</p>
      </button>
    </h3>
    <div id="flush-collapse10.1101/2023.11.27.568858" class="accordion-collapse collapse" aria-labelledby="flush-heading10.1101/2023.11.27.568858" data-bs-parent="#accordionFlushExample">
      <div class="accordion-body">
        <p class="author">Authors: Shen, Y.-R.; Zaballa, S.; Bech, X.; Sancho-Balsells, A.; Diaz-Cifuentes, C.; Seyit-Bremer, G.; Ballasch, I.; Alcazar, N.; Alberch, J.; Abad, M.; Serrano, M.; Klein, R.; Giralt, A.; del Toro, D.</p>
        <p class="info">Score: 16.8, Published: 2023-11-27 </p>
        <p class="info">DOI: <a href='https://doi.org/10.1101/2023.11.27.568858' target='https://doi.org/10.1101/2023.11.27.568858'> 10.1101/2023.11.27.568858</a></p>
        <p class="abstract">Yamanaka factors (YFs) can reverse some aging features in mammalian tissues, but their effects on the brain remain largely unexplored. Here, we induced YFs in the mouse brain in a controlled spatio-temporal manner in two different scenarios: brain development, and adult stages in the context of neurodegeneration. Embryonic induction of YFs perturbed cell identity of both progenitors and neurons, but transient and low-level expression is tolerated by these cells during development. Under these conditions, YFs induction led to expanded neurogenesis, increased number of upper cortical neurons, and enhanced motor and social behavior of adult mice. Additionally, controlled YF induction is tolerated by principal neurons in the adult dorsal hippocampus and prevented the development of several hallmarks of Alzheimers disease, including cognitive decline and altered molecular signatures, in the 5xFAD mouse model. Overall, these results highlight the powerful impact of YFs on neurogenesis and their potential use in brain disorders.

HighlightsO_LITransient Yamanaka factor (YF) expression during development expands neocortex
C_LIO_LIYF-treated mice show enhanced cognitive skills
C_LIO_LIIntermitent YF expression is tolerated by adult principal hippocampal neurons
C_LIO_LILong-term intermitent YF reprogramming is protective in an AD mouse model
C_LI</p>
      </div>
    </div>
  </div><div class="accordion-item">
    <h3 class="accordion-header" id="flush-heading10.1101/2023.11.16.567097">
      <button class="accordion-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapse10.1101/2023.11.16.567097" aria-expanded="false" aria-controls="flush-collapse10.1101/2023.11.16.567097">
        <p class="paperTitle">Unravelling the neural dynamics of hypnotic susceptibility: Aperiodic neural activity as a central feature of hypnosis</p>
      </button>
    </h3>
    <div id="flush-collapse10.1101/2023.11.16.567097" class="accordion-collapse collapse" aria-labelledby="flush-heading10.1101/2023.11.16.567097" data-bs-parent="#accordionFlushExample">
      <div class="accordion-body">
        <p class="author">Authors: Landry, M.; da Silva Castanheira, J.; Sackur, J.; Raz, A.; Ogez, D.; Rainville, P.; Jerbi, K.</p>
        <p class="info">Score: 35.0, Published: 2023-11-17 </p>
        <p class="info">DOI: <a href='https://doi.org/10.1101/2023.11.16.567097' target='https://doi.org/10.1101/2023.11.16.567097'> 10.1101/2023.11.16.567097</a></p>
        <p class="abstract">The ability for hypnotic responding is marked by great inter-individual variation in the population, while the neural underpinning of this variability remains elusive. The current work leveraged multivariate statistics and machine learning to probe the neural dynamics underlying inter-individual differences in hypnotic susceptibility. We assessed the efficacy of linear classifiers in distinguishing between high and low hypnotic susceptible individuals using neural features from resting-state electroencephalography (EEG) both pre- and post-hypnotic induction. Our focus encompassed both aperiodic and periodic components of the power spectrum, and graph theoretical measures derived from functional connectivity patterns. Several neural features from both pre- and post-induction significantly differentiated susceptibility levels, which underscores the complex dynamics of hypnotic phenomena. Based on model comparisons and feature ranking, we discerned the pre-induction aperiodic exponent as the primary discriminating neural feature, while periodic activity did not differ between groups. This novel finding not only resonates with the increasing emphasis on this neural component in broader EEG research but also promotes the idea that the primary neural distinction in hypnotic susceptibility is evident at baseline, even before hypnosis. Based on prevailing interpretation of aperiodicity in the EEG signal, our findings support the idea that hypnotic susceptibility might be an inherent trait reflected in the balance of cortical excitation and inhibition.

Significance StatementHypnotic phenomena reflect the ability to alter ones subjective experiences based on targeted verbal suggestions. While research has made strides in understanding the cognitive aspects underlying the variability in ones susceptibility to hypnosis, the brain correlates remain largely elusive. Addressing this gap, our study employs multivariate pattern classification and machine learning to predict hypnotic susceptibility. By recording electroencephalography (EEG) before and after hypnotic induction and analyzing diverse neurophysiological features, we identify several features that differentiate between high and low hypnotic susceptible individuals for both pre- and post-induction periods, which underscores the multifaceted nature of hypnotic phenomena. Our analysis revealed that the paramount discriminative feature is arrhythmic (i.e., non-oscillatory) EEG activity prior to the induction--a novel discovery in the field. This finding, that the chief EEG marker is observed even before hypnosis, aligns with the idea that hypnotic susceptibility represents an inherent trait reflecting the balance between excitation and inhibition in neural activity.</p>
      </div>
    </div>
  </div>
</div>









<script src="/js/bundle.min.js" defer></script>



  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2023 <a href="https://trxiv.yorks0n.com">TRxiv2</a></span>
    <span>
        Â· Made by Yorkson
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
