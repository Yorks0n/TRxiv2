<!DOCTYPE html>
<html lang="en" dir="auto">

<head>
<head>
    <title>neuroscience</title>
    <meta charset="utf-8">
    <meta name="description"
        content="Website meta description for google search results go here" />
    <meta name="dc.relation" content="https://trxiv.yorks0n.com" />
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="theme-color" content="#1A94D2" />

    

    
    
    
    <link rel="stylesheet" href="/css/main.min.css" media="screen">

</head>


<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>neuroscience | TRxiv2</title>
<meta name="keywords" content="">
<meta name="description" content="What can 1.8 billion regressions tell us about the pressures shaping high-level visual representation in brains and machines?
Authors: Conwell, C.; Prince, J. S.; Kay, K. N.; Alvarez, G. A.; Konkle, T.
Score: 104.8, Published: 2023-07-01 DOI: 10.1101/2022.03.28.485868
The rapid development and open-source release of highly performant computer vision models offers new potential for examining how different inductive biases impact representation learning and emergent alignment with the high-level human ventral visual system.">
<meta name="author" content="">
<link rel="canonical" href="https://trxiv.yorks0n.com/posts/neuroscience/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.904bd1e751cdd2a584fa6bed3fa1166dfd8ec9949ebfd0c4d69c5add5e17c23d.css" integrity="sha256-kEvR51HN0qWE&#43;mvtP6EWbf2OyZSev9DE1pxa3V4Xwj0=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js" integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG&#43;9vmJ0cTS&#43;ovo0FeA="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://trxiv.yorks0n.com/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://trxiv.yorks0n.com/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://trxiv.yorks0n.com/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://trxiv.yorks0n.com/apple-touch-icon.png">
<link rel="mask-icon" href="https://trxiv.yorks0n.com/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:title" content="neuroscience" />
<meta property="og:description" content="What can 1.8 billion regressions tell us about the pressures shaping high-level visual representation in brains and machines?
Authors: Conwell, C.; Prince, J. S.; Kay, K. N.; Alvarez, G. A.; Konkle, T.
Score: 104.8, Published: 2023-07-01 DOI: 10.1101/2022.03.28.485868
The rapid development and open-source release of highly performant computer vision models offers new potential for examining how different inductive biases impact representation learning and emergent alignment with the high-level human ventral visual system." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://trxiv.yorks0n.com/posts/neuroscience/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-07-16T10:39:40+00:00" />
<meta property="article:modified_time" content="2023-07-16T10:39:40+00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="neuroscience"/>
<meta name="twitter:description" content="What can 1.8 billion regressions tell us about the pressures shaping high-level visual representation in brains and machines?
Authors: Conwell, C.; Prince, J. S.; Kay, K. N.; Alvarez, G. A.; Konkle, T.
Score: 104.8, Published: 2023-07-01 DOI: 10.1101/2022.03.28.485868
The rapid development and open-source release of highly performant computer vision models offers new potential for examining how different inductive biases impact representation learning and emergent alignment with the high-level human ventral visual system."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Posts",
      "item": "https://trxiv.yorks0n.com/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  3 ,
      "name": "neuroscience",
      "item": "https://trxiv.yorks0n.com/posts/neuroscience/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "neuroscience",
  "name": "neuroscience",
  "description": "What can 1.8 billion regressions tell us about the pressures shaping high-level visual representation in brains and machines?\nAuthors: Conwell, C.; Prince, J. S.; Kay, K. N.; Alvarez, G. A.; Konkle, T.\nScore: 104.8, Published: 2023-07-01 DOI: 10.1101/2022.03.28.485868\nThe rapid development and open-source release of highly performant computer vision models offers new potential for examining how different inductive biases impact representation learning and emergent alignment with the high-level human ventral visual system.",
  "keywords": [
    
  ],
  "articleBody": " What can 1.8 billion regressions tell us about the pressures shaping high-level visual representation in brains and machines?\nAuthors: Conwell, C.; Prince, J. S.; Kay, K. N.; Alvarez, G. A.; Konkle, T.\nScore: 104.8, Published: 2023-07-01 DOI: 10.1101/2022.03.28.485868\nThe rapid development and open-source release of highly performant computer vision models offers new potential for examining how different inductive biases impact representation learning and emergent alignment with the high-level human ventral visual system. Here, we assess a diverse set of 224 models, curated to enable controlled comparison of different model properties, testing their brain predictivity using large-scale functional magnetic resonance imaging data. We find that models with qualitatively different architectures (e.g. CNNs versus Transformers) and markedly different task objectives (e.g. purely visual contrastive learning versus vision-language alignment) achieve near equivalent degrees of brain predictivity, when other factors are held constant. Instead, variation across model visual training diets yields the largest, most consistent effect on emergent brain predictivity. Overarching model properties commonly suspected to increase brain predictivity (e.g. greater effective dimensionality; learnable parameter count) were not robust indicators across this more extensive survey. We highlight that standard model-to-brain linear re-weighting methods may be too flexible, as most performant models have very similar brain-predictivity scores, despite significant variation in their underlying representations. Broadly, our findings point to the importance of visual diet, challenge common assumptions about the methods used to link models to brains, and more concretely outline future directions for leveraging the full diversity of existing open-source models as tools to probe the common computational principles underlying biological and artificial visual systems.\nCompetition between stochastic neuropeptide signals calibrates the rate of satiation\nAuthors: Zhang, S. X.; Kim, A.; Madara, J. C.; Zhu, P. K.; Christenson, L. F.; Lutas, A.; Kalugin, P. N.; Jin, Y.; Paul, A.; Tian, L.; Lowell, B. B.; Andermann, M. L.\nScore: 53.7, Published: 2023-07-12 DOI: 10.1101/2023.07.11.548551\nWe investigated how transmission of hunger- and satiety-promoting neuropeptides, NPY and MSH, is integrated at the level of intracellular signaling to control feeding. Receptors for these peptides use the second messenger cAMP, but the messengers spatiotemporal dynamics and role in energy balance are controversial. We show that AgRP axon stimulation in the paraventricular hypothalamus evokes probabilistic and spatially restricted NPY release that triggers stochastic cAMP decrements in downstream MC4R-expressing neurons (PVHMC4R). Meanwhile, POMC axon stimulation triggers stochastic, MSH-dependent cAMP increments. NPY and MSH competitively control cAMP, as reflected by hunger-state-dependent differences in the amplitude and persistence of cAMP transients evoked by each peptide. During feeding bouts, elevated MSH release and suppressed NPY release cooperatively sustain elevated cAMP in PVHMC4R neurons, thereby potentiating feeding-related excitatory inputs and promoting satiation across minutes. Our findings highlight how state-dependent integration of opposing, quantal peptidergic events by a common biochemical target calibrates energy intake.\nCooperative thalamocortical circuit mechanism for sensory prediction errors\nAuthors: Furutachi, S.; Franklin, A. D.; Mrsic-Flogel, T. D.; Hofer, S. B.\nScore: 49.4, Published: 2023-07-12 DOI: 10.1101/2023.07.12.548664\nThe brain functions as a prediction machine, utilizing an internal model of the world to anticipate sensations and the outcomes of our actions. Discrepancies between expected and actual events, referred to as prediction errors, are leveraged to update the internal model and guide our attention towards unexpected events. Despite the importance of prediction error signals for various neural computations across multiple brain regions, surprisingly little is known about the neural circuit mechanisms responsible for their implementation. Here we describe a thalamocortical disinhibitory circuit required for generating sensory prediction errors in mouse primary visual cortex (V1). Using calcium imaging with optogenetic manipulations as mice traverse a familiar virtual environment, we show that violation of animals' predictions by an unexpected visual stimulus preferentially boosts responses of layer 2/3 V1 neurons most selective for that stimulus. Prediction errors specifically amplify the unexpected visual input, rather than representing a non-specific surprise or difference signal about how the visual input deviates from animals' predictions. Selective amplification of unexpected visual input is implemented by a cooperative mechanism requiring thalamic input from the pulvinar, and cortical vasoactive-intestinal-peptide-expressing (VIP) inhibitory interneurons. In response to prediction errors, VIP neurons inhibit a specific subpopulation of somatostatin-expressing (SOM) inhibitory interneurons that gate excitatory pulvinar input to V1, resulting in specific pulvinar-driven response-amplification of the most stimulus-selective neurons in V1. Therefore, the brain prioritizes unpredicted sensory information by selectively increasing the salience of unpredicted sensory features through the synergistic interaction of thalamic input and neocortical disinhibitory circuits.\nA Versatile and Open Source One- and Two-Photon Light-Sheet Microscope Design\nAuthors: Panier, T.; Migault, G.; Hubert, A.; Trentesaux, H.; Beaudou, B.; Debregeas, G.; Bormuth, V.\nScore: 49.0, Published: 2023-07-11 DOI: 10.1101/2023.07.10.548107\nTwo-photon light sheet microscopy offers great potential for a range of biological applications, but its practical implementation is impeded by the high cost of laser sources, the complexity of construction, and the challenges associated with adapting to existing microscope setups. Here, we release an open-source design that addresses these limitations by providing detailed building instructions for the transformation of a brightfield microscope into a versatile one- and two-photon light sheet system. Our design incorporates a specially designed broadband hollow core fiber, enabling the simultaneous utilization of an expansive pulsed laser source from another setup alongside a visible laser. This integration allows for uncompromised image resolution and speed. Furthermore, the design reduces the complexity of construction, alignment, and overall cost, thereby significantly enhancing the accessibility of this technology (https://github.com/LJPZebra/OLU).\nPredictions and errors are distinctly represented across V1 layers\nAuthors: Thomas, E. R.; Haarsma, J.; Nicholson, J.; Yon, D.; Kok, P.; Press, C.\nScore: 47.3, Published: 2023-07-12 DOI: 10.1101/2023.07.11.548408\nPredictive processing frameworks of cortical functioning propose that neural populations in different cortical layers serve distinct roles in representing the world. There are distinct testable theories within this framework that we examined with a 7T fMRI study, where we contrasted responses in primary visual cortex (V1) to expected (75% likely) and unexpected (25%) Gabor orientations. Multivariate decoding analyses revealed an interaction between expectation and layer, such that expected events could be decoded with comparable accuracy across layers, while unexpected events could only be decoded in superficial laminae. These results are in line with predictive processing accounts where expected virtual input is injected into deep layers, while superficial layers process the error with respect to expected signals. While this account of cortical processing has been popular for decades, such distinctions have not previously been demonstrated in the human sensory brain. We discuss how both prediction and error processes may operate together to shape our unitary perceptual experiences.\nNeural signatures of natural behavior in socializing macaques\nAuthors: Testard, C.; Tremblay, S.; Parodi, F.; DiTullio, R. W.; Acevedo-Ithier, A.; Gardiner, K.; Kording, K. P.; Platt, M.\nScore: 32.5, Published: 2023-07-07 DOI: 10.1101/2023.07.05.547833\nOur understanding of the neurobiology of primate behavior largely derives from artificial tasks in highly-controlled laboratory settings, overlooking most natural behaviors primate brains evolved to produce1. In particular, how primates navigate the multidimensional social relationships that structure daily life2 and shape survival and reproductive success3 remains largely unexplored at the single neuron level. Here, we combine ethological analysis with new wireless recording technologies to uncover neural signatures of natural behavior in unrestrained, socially interacting pairs of rhesus macaques within a larger colony. Population decoding of single neuron activity in prefrontal and temporal cortex unveiled robust encoding of 24 species-typical behaviors, which was strongly modulated by the presence and identity of surrounding monkeys. Male-female partners demonstrated near-perfect reciprocity in grooming, a key behavioral mechanism supporting friendships and alliances2, and neural activity maintained a running account of these social investments. When confronted with an aggressive intruder, behavioral and neural population responses reflected empathy and were buffered by the presence of a partner. Surprisingly, neural signatures in prefrontal and temporal cortex were largely indistinguishable and irreducible to visual and motor contingencies. By employing an ethological approach to the study of primate neurobiology, we reveal a highly-distributed neurophysiological record of social dynamics, a potential computational foundation supporting communal life in primate societies, including our own.\nMolecular signature and functional properties of human pluripotent stem cell-derived brain pericytes\nAuthors: Bosworth, A.; Griffin, C.; Chakhoyan, A.; Sagare, A. P.; Nelson, A. R.; Wang, Y.; Kisler, K.; Montagne, A.; Clementel, V.; TCW, J.; Rust, R.; Coba, M.; Zlokovic, B.\nScore: 29.3, Published: 2023-06-28 DOI: 10.1101/2023.06.26.546577\nBrain pericytes maintain blood-brain barrier (BBB), secrete neurotrophic factors and clear toxic proteins. Their loss in neurological disorders leads to BBB breakdown, neuronal dysfunction, and cognitive decline. Therefore, cell therapy to replace lost pericytes holds potential to restore impaired cerebrovascular and brain functions. Here, we show by a quantitative analysis of 8,344 proteins and 20,572 phosphopeptides that human iPSC-derived brain pericytes (iPSC-PC) share 96% of total proteins and 98% of protein phosphorylation sites with primary human brain pericytes. This includes cell adhesion and tight junction proteins, transcription factors, and different protein kinase families of the human kinome. In pericyte-deficient mice, iPSC-PC home to host brain capillaries to form hybrid human-mouse microvessels. They repair BBB leaks and protect against neuron loss, which we show requires PDGRFB and pleiotrophin. They also clear Alzheimers amyloid-{beta} and tau neurotoxins via lipoprotein receptor. Thus, iPSC-PC may offer a valuable replacement therapy for pericyte-deficient neurological disorders.\nExpansion-assisted selective plane illumination microscopy for nanoscale imaging of centimeter-scale tissues\nAuthors: Glaser, A.; Chandrashekar, J.; Vasquez, J.; Arshadi, C.; Ouellette, N.; Jiang, X.; Baka, J.; Kovacs, G.; Woodard, M.; Seshamani, S.; Cao, K.; Clack, N.; Recknagel, A.; Grim, A.; Balaram, P.; Turschak, E.; Liddell, A.; Rohde, J.; Hellevik, A.; Takasaki, K.; Erion Barner, L.; Logsdon, M.; Chronopoulos, C.; de Vries, S.; Ting, J.; Perlmutter, S.; Kalmbach, B.; Dembrow, N.; Reid, R. C.; Feng, D.; Svoboda, K.\nScore: 375.5, Published: 2023-06-27 DOI: 10.1101/2023.06.08.544277\nRecent advances in tissue processing, labeling, and fluorescence microscopy are providing unprecedented views of the structure of cells and tissues at sub-diffraction resolutions and near single molecule sensitivity, driving discoveries in diverse fields of biology, including neuroscience. Biological tissue is organized over scales of nanometers to centimeters. Harnessing molecular imaging across three-dimensional samples on this scale requires new types of microscopes with larger fields of view and working distance, as well as higher imaging throughput. We present a new expansion-assisted selective plane illumination microscope (ExA-SPIM) with diffraction-limited and aberration-free performance over a large field of view (85 mm2) and working distance (35 mm). Combined with new tissue clearing and expansion methods, the microscope allows nanoscale imaging of centimeter-scale samples, including entire mouse brains, with diffraction-limited resolutions and high contrast without sectioning. We illustrate ExA-SPIM by reconstructing individual neurons across the mouse brain, imaging cortico-spinal neurons in the macaque motor cortex, and tracing axons in human white matter.\nVirally induced CRISPR/Cas9-based knock-in of fluorescent albumin allows long-term visualization of cerebral circulation in infant and adult mice\nAuthors: Vittani, M.; Knak, P. A. G.; Fukuda, M.; Nagao, M.; Wang, X.; Kjaerby, C.; Konno, A.; Hirai, H.; Nedergaard, M.; Hirase, H.\nScore: 20.1, Published: 2023-07-10 DOI: 10.1101/2023.07.10.548084\nAlbumin, a protein produced by liver hepatocytes, represents the most abundant protein in blood plasma. We have previously engineered a liver-targeting adeno-associated viral vector (AAV) that expresses fluorescent protein-tagged albumin to visualize blood plasma in mice. While this approach is versatile for imaging in adult mice, transgene expression vanishes when AAV is administered in neonates due to dilution of the episomal AAV genome in the rapidly growing liver. Here, we use CRISPR/Cas9 genome editing to insert the fluorescent protein mNeonGreen (mNG) gene into the albumin (Alb) locus of hepatocytes to produce fluorescently labeled albumin (Alb-mNG). We constructed a CRISPR AAV that includes [~]1 kb homologous arms around Alb exon 14 to express Alb-mNG. Subcutaneous injection of this AAV with AAV-CMV-Cas9 in postnatal day 3 mice resulted in two-photon visualization of the cerebral cortex vasculature within ten days. The expression levels of Alb-mNG were persistent for at least three months and were so robust that vasomotion and capillary blood flow could be assessed transcranially in early postnatal mice. This knock-in approach provides powerful means for micro- and macroscopic imaging of cerebral vascular dynamics in postnatal and adult mice.\nThe Integrated Stress Response effector GADD34 is repurposed by neurons to promote stimulus-induced translation\nAuthors: Oliveira, M. M.; Mohamed, M. S.; Elder, M. K.; Banegas-Morales, K.; Mamcarz, M.; Lu, E. H.; Golhan, E. A. N.; Navrange, N.; Chatterjee, S.; Abel, T.; Klann, E.\nScore: 18.9, Published: 2023-07-12 DOI: 10.1101/2023.07.12.548606\nNeuronal protein synthesis is required for long-lasting plasticity and long-term memory consolidation. Dephosphorylation of eukaryotic initiation factor 2 is one of the key translational control events that is required to increase de novo protein synthesis that underlies long-lasting plasticity and memory consolidation. Here, we interrogate the molecular pathways of translational control that are triggered by stimulation of neurons with brain-derived neurotrophic factor (BDNF), which results in eIF2 dephosphorylation and de novo protein synthesis. Primary rodent neurons exposed to BDNF displayed elevated translation, but not transcription, of GADD34, which facilitates eIF2 dephosphorylation and subsequent de novo protein synthesis. Furthermore, GADD34 requires G-actin generated by cofilin to dephosphorylate eIF2 and enhance protein synthesis. Finally, GADD34 is required for the BDNF-induced translation of synaptic plasticity-related proteins. Overall, we provide evidence that neurons repurpose GADD34, an effector of the Integrated Stress Response, as an orchestrator of rapid increases in eIF2-dependent translation in response to plasticity-inducing stimuli.\n",
  "wordCount" : "2189",
  "inLanguage": "en",
  "datePublished": "2023-07-16T10:39:40Z",
  "dateModified": "2023-07-16T10:39:40Z",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://trxiv.yorks0n.com/posts/neuroscience/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "TRxiv2",
    "logo": {
      "@type": "ImageObject",
      "url": "https://trxiv.yorks0n.com/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://trxiv.yorks0n.com" accesskey="h" title="TRxiv2 (Alt + H)">TRxiv2</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
        </ul>
    </nav>
</header>
<main class="main">
<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title">
      neuroscience
    </h1>
    <div class="post-meta"><span>updated on July 16, 2023</span>

</div>
  </header> 
  <div class="post-content"><div class="accordion accordion-flush" id="accordionFlushExample"><div class="accordion-item">
    <h3 class="accordion-header" id="flush-heading10.1101/2022.03.28.485868">
      <button class="accordion-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapse10.1101/2022.03.28.485868" aria-expanded="false" aria-controls="flush-collapse10.1101/2022.03.28.485868">
        <p class="paperTitle">What can 1.8 billion regressions tell us about the pressures shaping high-level visual representation in brains and machines?</p>
      </button>
    </h3>
    <div id="flush-collapse10.1101/2022.03.28.485868" class="accordion-collapse collapse" aria-labelledby="flush-heading10.1101/2022.03.28.485868" data-bs-parent="#accordionFlushExample">
      <div class="accordion-body">
        <p class="author">Authors: Conwell, C.; Prince, J. S.; Kay, K. N.; Alvarez, G. A.; Konkle, T.</p>
        <p class="info">Score: 104.8, Published: 2023-07-01 </p>
        <p class="info">DOI: <a href='https://doi.org/10.1101/2022.03.28.485868' target='https://doi.org/10.1101/2022.03.28.485868'> 10.1101/2022.03.28.485868</a></p>
        <p class="abstract">The rapid development and open-source release of highly performant computer vision models offers new potential for examining how different inductive biases impact representation learning and emergent alignment with the high-level human ventral visual system. Here, we assess a diverse set of 224 models, curated to enable controlled comparison of different model properties, testing their brain predictivity using large-scale functional magnetic resonance imaging data. We find that models with qualitatively different architectures (e.g. CNNs versus Transformers) and markedly different task objectives (e.g. purely visual contrastive learning versus vision-language alignment) achieve near equivalent degrees of brain predictivity, when other factors are held constant. Instead, variation across model visual training diets yields the largest, most consistent effect on emergent brain predictivity. Overarching model properties commonly suspected to increase brain predictivity (e.g. greater effective dimensionality; learnable parameter count) were not robust indicators across this more extensive survey. We highlight that standard model-to-brain linear re-weighting methods may be too flexible, as most performant models have very similar brain-predictivity scores, despite significant variation in their underlying representations. Broadly, our findings point to the importance of visual diet, challenge common assumptions about the methods used to link models to brains, and more concretely outline future directions for leveraging the full diversity of existing open-source models as tools to probe the common computational principles underlying biological and artificial visual systems.</p>
      </div>
    </div>
  </div><div class="accordion-item">
    <h3 class="accordion-header" id="flush-heading10.1101/2023.07.11.548551">
      <button class="accordion-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapse10.1101/2023.07.11.548551" aria-expanded="false" aria-controls="flush-collapse10.1101/2023.07.11.548551">
        <p class="paperTitle">Competition between stochastic neuropeptide signals calibrates the rate of satiation</p>
      </button>
    </h3>
    <div id="flush-collapse10.1101/2023.07.11.548551" class="accordion-collapse collapse" aria-labelledby="flush-heading10.1101/2023.07.11.548551" data-bs-parent="#accordionFlushExample">
      <div class="accordion-body">
        <p class="author">Authors: Zhang, S. X.; Kim, A.; Madara, J. C.; Zhu, P. K.; Christenson, L. F.; Lutas, A.; Kalugin, P. N.; Jin, Y.; Paul, A.; Tian, L.; Lowell, B. B.; Andermann, M. L.</p>
        <p class="info">Score: 53.7, Published: 2023-07-12 </p>
        <p class="info">DOI: <a href='https://doi.org/10.1101/2023.07.11.548551' target='https://doi.org/10.1101/2023.07.11.548551'> 10.1101/2023.07.11.548551</a></p>
        <p class="abstract">We investigated how transmission of hunger- and satiety-promoting neuropeptides, NPY and MSH, is integrated at the level of intracellular signaling to control feeding. Receptors for these peptides use the second messenger cAMP, but the messengers spatiotemporal dynamics and role in energy balance are controversial. We show that AgRP axon stimulation in the paraventricular hypothalamus evokes probabilistic and spatially restricted NPY release that triggers stochastic cAMP decrements in downstream MC4R-expressing neurons (PVHMC4R). Meanwhile, POMC axon stimulation triggers stochastic, MSH-dependent cAMP increments. NPY and MSH competitively control cAMP, as reflected by hunger-state-dependent differences in the amplitude and persistence of cAMP transients evoked by each peptide. During feeding bouts, elevated MSH release and suppressed NPY release cooperatively sustain elevated cAMP in PVHMC4R neurons, thereby potentiating feeding-related excitatory inputs and promoting satiation across minutes. Our findings highlight how state-dependent integration of opposing, quantal peptidergic events by a common biochemical target calibrates energy intake.</p>
      </div>
    </div>
  </div><div class="accordion-item">
    <h3 class="accordion-header" id="flush-heading10.1101/2023.07.12.548664">
      <button class="accordion-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapse10.1101/2023.07.12.548664" aria-expanded="false" aria-controls="flush-collapse10.1101/2023.07.12.548664">
        <p class="paperTitle">Cooperative thalamocortical circuit mechanism for sensory prediction errors</p>
      </button>
    </h3>
    <div id="flush-collapse10.1101/2023.07.12.548664" class="accordion-collapse collapse" aria-labelledby="flush-heading10.1101/2023.07.12.548664" data-bs-parent="#accordionFlushExample">
      <div class="accordion-body">
        <p class="author">Authors: Furutachi, S.; Franklin, A. D.; Mrsic-Flogel, T. D.; Hofer, S. B.</p>
        <p class="info">Score: 49.4, Published: 2023-07-12 </p>
        <p class="info">DOI: <a href='https://doi.org/10.1101/2023.07.12.548664' target='https://doi.org/10.1101/2023.07.12.548664'> 10.1101/2023.07.12.548664</a></p>
        <p class="abstract">The brain functions as a prediction machine, utilizing an internal model of the world to anticipate sensations and the outcomes of our actions. Discrepancies between expected and actual events, referred to as prediction errors, are leveraged to update the internal model and guide our attention towards unexpected events. Despite the importance of prediction error signals for various neural computations across multiple brain regions, surprisingly little is known about the neural circuit mechanisms responsible for their implementation. Here we describe a thalamocortical disinhibitory circuit required for generating sensory prediction errors in mouse primary visual cortex (V1). Using calcium imaging with optogenetic manipulations as mice traverse a familiar virtual environment, we show that violation of animals&#39; predictions by an unexpected visual stimulus preferentially boosts responses of layer 2/3 V1 neurons most selective for that stimulus. Prediction errors specifically amplify the unexpected visual input, rather than representing a non-specific surprise or difference signal about how the visual input deviates from animals&#39; predictions. Selective amplification of unexpected visual input is implemented by a cooperative mechanism requiring thalamic input from the pulvinar, and cortical vasoactive-intestinal-peptide-expressing (VIP) inhibitory interneurons. In response to prediction errors, VIP neurons inhibit a specific subpopulation of somatostatin-expressing (SOM) inhibitory interneurons that gate excitatory pulvinar input to V1, resulting in specific pulvinar-driven response-amplification of the most stimulus-selective neurons in V1. Therefore, the brain prioritizes unpredicted sensory information by selectively increasing the salience of unpredicted sensory features through the synergistic interaction of thalamic input and neocortical disinhibitory circuits.</p>
      </div>
    </div>
  </div><div class="accordion-item">
    <h3 class="accordion-header" id="flush-heading10.1101/2023.07.10.548107">
      <button class="accordion-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapse10.1101/2023.07.10.548107" aria-expanded="false" aria-controls="flush-collapse10.1101/2023.07.10.548107">
        <p class="paperTitle">A Versatile and Open Source One- and Two-Photon Light-Sheet Microscope Design</p>
      </button>
    </h3>
    <div id="flush-collapse10.1101/2023.07.10.548107" class="accordion-collapse collapse" aria-labelledby="flush-heading10.1101/2023.07.10.548107" data-bs-parent="#accordionFlushExample">
      <div class="accordion-body">
        <p class="author">Authors: Panier, T.; Migault, G.; Hubert, A.; Trentesaux, H.; Beaudou, B.; Debregeas, G.; Bormuth, V.</p>
        <p class="info">Score: 49.0, Published: 2023-07-11 </p>
        <p class="info">DOI: <a href='https://doi.org/10.1101/2023.07.10.548107' target='https://doi.org/10.1101/2023.07.10.548107'> 10.1101/2023.07.10.548107</a></p>
        <p class="abstract">Two-photon light sheet microscopy offers great potential for a range of biological applications, but its practical implementation is impeded by the high cost of laser sources, the complexity of construction, and the challenges associated with adapting to existing microscope setups. Here, we release an open-source design that addresses these limitations by providing detailed building instructions for the transformation of a brightfield microscope into a versatile one- and two-photon light sheet system. Our design incorporates a specially designed broadband hollow core fiber, enabling the simultaneous utilization of an expansive pulsed laser source from another setup alongside a visible laser. This integration allows for uncompromised image resolution and speed. Furthermore, the design reduces the complexity of construction, alignment, and overall cost, thereby significantly enhancing the accessibility of this technology (https://github.com/LJPZebra/OLU).</p>
      </div>
    </div>
  </div><div class="accordion-item">
    <h3 class="accordion-header" id="flush-heading10.1101/2023.07.11.548408">
      <button class="accordion-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapse10.1101/2023.07.11.548408" aria-expanded="false" aria-controls="flush-collapse10.1101/2023.07.11.548408">
        <p class="paperTitle">Predictions and errors are distinctly represented across V1 layers</p>
      </button>
    </h3>
    <div id="flush-collapse10.1101/2023.07.11.548408" class="accordion-collapse collapse" aria-labelledby="flush-heading10.1101/2023.07.11.548408" data-bs-parent="#accordionFlushExample">
      <div class="accordion-body">
        <p class="author">Authors: Thomas, E. R.; Haarsma, J.; Nicholson, J.; Yon, D.; Kok, P.; Press, C.</p>
        <p class="info">Score: 47.3, Published: 2023-07-12 </p>
        <p class="info">DOI: <a href='https://doi.org/10.1101/2023.07.11.548408' target='https://doi.org/10.1101/2023.07.11.548408'> 10.1101/2023.07.11.548408</a></p>
        <p class="abstract">Predictive processing frameworks of cortical functioning propose that neural populations in different cortical layers serve distinct roles in representing the world. There are distinct testable theories within this framework that we examined with a 7T fMRI study, where we contrasted responses in primary visual cortex (V1) to expected (75% likely) and unexpected (25%) Gabor orientations. Multivariate decoding analyses revealed an interaction between expectation and layer, such that expected events could be decoded with comparable accuracy across layers, while unexpected events could only be decoded in superficial laminae. These results are in line with predictive processing accounts where expected virtual input is injected into deep layers, while superficial layers process the error with respect to expected signals. While this account of cortical processing has been popular for decades, such distinctions have not previously been demonstrated in the human sensory brain. We discuss how both prediction and error processes may operate together to shape our unitary perceptual experiences.</p>
      </div>
    </div>
  </div><div class="accordion-item">
    <h3 class="accordion-header" id="flush-heading10.1101/2023.07.05.547833">
      <button class="accordion-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapse10.1101/2023.07.05.547833" aria-expanded="false" aria-controls="flush-collapse10.1101/2023.07.05.547833">
        <p class="paperTitle">Neural signatures of natural behavior in socializing macaques</p>
      </button>
    </h3>
    <div id="flush-collapse10.1101/2023.07.05.547833" class="accordion-collapse collapse" aria-labelledby="flush-heading10.1101/2023.07.05.547833" data-bs-parent="#accordionFlushExample">
      <div class="accordion-body">
        <p class="author">Authors: Testard, C.; Tremblay, S.; Parodi, F.; DiTullio, R. W.; Acevedo-Ithier, A.; Gardiner, K.; Kording, K. P.; Platt, M.</p>
        <p class="info">Score: 32.5, Published: 2023-07-07 </p>
        <p class="info">DOI: <a href='https://doi.org/10.1101/2023.07.05.547833' target='https://doi.org/10.1101/2023.07.05.547833'> 10.1101/2023.07.05.547833</a></p>
        <p class="abstract">Our understanding of the neurobiology of primate behavior largely derives from artificial tasks in highly-controlled laboratory settings, overlooking most natural behaviors primate brains evolved to produce1. In particular, how primates navigate the multidimensional social relationships that structure daily life2 and shape survival and reproductive success3 remains largely unexplored at the single neuron level. Here, we combine ethological analysis with new wireless recording technologies to uncover neural signatures of natural behavior in unrestrained, socially interacting pairs of rhesus macaques within a larger colony. Population decoding of single neuron activity in prefrontal and temporal cortex unveiled robust encoding of 24 species-typical behaviors, which was strongly modulated by the presence and identity of surrounding monkeys. Male-female partners demonstrated near-perfect reciprocity in grooming, a key behavioral mechanism supporting friendships and alliances2, and neural activity maintained a running account of these social investments. When confronted with an aggressive intruder, behavioral and neural population responses reflected empathy and were buffered by the presence of a partner. Surprisingly, neural signatures in prefrontal and temporal cortex were largely indistinguishable and irreducible to visual and motor contingencies. By employing an ethological approach to the study of primate neurobiology, we reveal a highly-distributed neurophysiological record of social dynamics, a potential computational foundation supporting communal life in primate societies, including our own.</p>
      </div>
    </div>
  </div><div class="accordion-item">
    <h3 class="accordion-header" id="flush-heading10.1101/2023.06.26.546577">
      <button class="accordion-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapse10.1101/2023.06.26.546577" aria-expanded="false" aria-controls="flush-collapse10.1101/2023.06.26.546577">
        <p class="paperTitle">Molecular signature and functional properties of human pluripotent stem cell-derived brain pericytes</p>
      </button>
    </h3>
    <div id="flush-collapse10.1101/2023.06.26.546577" class="accordion-collapse collapse" aria-labelledby="flush-heading10.1101/2023.06.26.546577" data-bs-parent="#accordionFlushExample">
      <div class="accordion-body">
        <p class="author">Authors: Bosworth, A.; Griffin, C.; Chakhoyan, A.; Sagare, A. P.; Nelson, A. R.; Wang, Y.; Kisler, K.; Montagne, A.; Clementel, V.; TCW, J.; Rust, R.; Coba, M.; Zlokovic, B.</p>
        <p class="info">Score: 29.3, Published: 2023-06-28 </p>
        <p class="info">DOI: <a href='https://doi.org/10.1101/2023.06.26.546577' target='https://doi.org/10.1101/2023.06.26.546577'> 10.1101/2023.06.26.546577</a></p>
        <p class="abstract">Brain pericytes maintain blood-brain barrier (BBB), secrete neurotrophic factors and clear toxic proteins. Their loss in neurological disorders leads to BBB breakdown, neuronal dysfunction, and cognitive decline. Therefore, cell therapy to replace lost pericytes holds potential to restore impaired cerebrovascular and brain functions. Here, we show by a quantitative analysis of 8,344 proteins and 20,572 phosphopeptides that human iPSC-derived brain pericytes (iPSC-PC) share 96% of total proteins and 98% of protein phosphorylation sites with primary human brain pericytes. This includes cell adhesion and tight junction proteins, transcription factors, and different protein kinase families of the human kinome. In pericyte-deficient mice, iPSC-PC home to host brain capillaries to form hybrid human-mouse microvessels. They repair BBB leaks and protect against neuron loss, which we show requires PDGRFB and pleiotrophin. They also clear Alzheimers amyloid-{beta} and tau neurotoxins via lipoprotein receptor. Thus, iPSC-PC may offer a valuable replacement therapy for pericyte-deficient neurological disorders.</p>
      </div>
    </div>
  </div><div class="accordion-item">
    <h3 class="accordion-header" id="flush-heading10.1101/2023.06.08.544277">
      <button class="accordion-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapse10.1101/2023.06.08.544277" aria-expanded="false" aria-controls="flush-collapse10.1101/2023.06.08.544277">
        <p class="paperTitle">Expansion-assisted selective plane illumination microscopy for nanoscale imaging of centimeter-scale tissues</p>
      </button>
    </h3>
    <div id="flush-collapse10.1101/2023.06.08.544277" class="accordion-collapse collapse" aria-labelledby="flush-heading10.1101/2023.06.08.544277" data-bs-parent="#accordionFlushExample">
      <div class="accordion-body">
        <p class="author">Authors: Glaser, A.; Chandrashekar, J.; Vasquez, J.; Arshadi, C.; Ouellette, N.; Jiang, X.; Baka, J.; Kovacs, G.; Woodard, M.; Seshamani, S.; Cao, K.; Clack, N.; Recknagel, A.; Grim, A.; Balaram, P.; Turschak, E.; Liddell, A.; Rohde, J.; Hellevik, A.; Takasaki, K.; Erion Barner, L.; Logsdon, M.; Chronopoulos, C.; de Vries, S.; Ting, J.; Perlmutter, S.; Kalmbach, B.; Dembrow, N.; Reid, R. C.; Feng, D.; Svoboda, K.</p>
        <p class="info">Score: 375.5, Published: 2023-06-27 </p>
        <p class="info">DOI: <a href='https://doi.org/10.1101/2023.06.08.544277' target='https://doi.org/10.1101/2023.06.08.544277'> 10.1101/2023.06.08.544277</a></p>
        <p class="abstract">Recent advances in tissue processing, labeling, and fluorescence microscopy are providing unprecedented views of the structure of cells and tissues at sub-diffraction resolutions and near single molecule sensitivity, driving discoveries in diverse fields of biology, including neuroscience. Biological tissue is organized over scales of nanometers to centimeters. Harnessing molecular imaging across three-dimensional samples on this scale requires new types of microscopes with larger fields of view and working distance, as well as higher imaging throughput. We present a new expansion-assisted selective plane illumination microscope (ExA-SPIM) with diffraction-limited and aberration-free performance over a large field of view (85 mm2) and working distance (35 mm). Combined with new tissue clearing and expansion methods, the microscope allows nanoscale imaging of centimeter-scale samples, including entire mouse brains, with diffraction-limited resolutions and high contrast without sectioning. We illustrate ExA-SPIM by reconstructing individual neurons across the mouse brain, imaging cortico-spinal neurons in the macaque motor cortex, and tracing axons in human white matter.</p>
      </div>
    </div>
  </div><div class="accordion-item">
    <h3 class="accordion-header" id="flush-heading10.1101/2023.07.10.548084">
      <button class="accordion-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapse10.1101/2023.07.10.548084" aria-expanded="false" aria-controls="flush-collapse10.1101/2023.07.10.548084">
        <p class="paperTitle">Virally induced CRISPR/Cas9-based knock-in of fluorescent albumin allows long-term visualization of cerebral circulation in infant and adult mice</p>
      </button>
    </h3>
    <div id="flush-collapse10.1101/2023.07.10.548084" class="accordion-collapse collapse" aria-labelledby="flush-heading10.1101/2023.07.10.548084" data-bs-parent="#accordionFlushExample">
      <div class="accordion-body">
        <p class="author">Authors: Vittani, M.; Knak, P. A. G.; Fukuda, M.; Nagao, M.; Wang, X.; Kjaerby, C.; Konno, A.; Hirai, H.; Nedergaard, M.; Hirase, H.</p>
        <p class="info">Score: 20.1, Published: 2023-07-10 </p>
        <p class="info">DOI: <a href='https://doi.org/10.1101/2023.07.10.548084' target='https://doi.org/10.1101/2023.07.10.548084'> 10.1101/2023.07.10.548084</a></p>
        <p class="abstract">Albumin, a protein produced by liver hepatocytes, represents the most abundant protein in blood plasma. We have previously engineered a liver-targeting adeno-associated viral vector (AAV) that expresses fluorescent protein-tagged albumin to visualize blood plasma in mice. While this approach is versatile for imaging in adult mice, transgene expression vanishes when AAV is administered in neonates due to dilution of the episomal AAV genome in the rapidly growing liver. Here, we use CRISPR/Cas9 genome editing to insert the fluorescent protein mNeonGreen (mNG) gene into the albumin (Alb) locus of hepatocytes to produce fluorescently labeled albumin (Alb-mNG). We constructed a CRISPR AAV that includes [~]1 kb homologous arms around Alb exon 14 to express Alb-mNG. Subcutaneous injection of this AAV with AAV-CMV-Cas9 in postnatal day 3 mice resulted in two-photon visualization of the cerebral cortex vasculature within ten days. The expression levels of Alb-mNG were persistent for at least three months and were so robust that vasomotion and capillary blood flow could be assessed transcranially in early postnatal mice. This knock-in approach provides powerful means for micro- and macroscopic imaging of cerebral vascular dynamics in postnatal and adult mice.</p>
      </div>
    </div>
  </div><div class="accordion-item">
    <h3 class="accordion-header" id="flush-heading10.1101/2023.07.12.548606">
      <button class="accordion-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapse10.1101/2023.07.12.548606" aria-expanded="false" aria-controls="flush-collapse10.1101/2023.07.12.548606">
        <p class="paperTitle">The Integrated Stress Response effector GADD34 is repurposed by neurons to promote stimulus-induced translation</p>
      </button>
    </h3>
    <div id="flush-collapse10.1101/2023.07.12.548606" class="accordion-collapse collapse" aria-labelledby="flush-heading10.1101/2023.07.12.548606" data-bs-parent="#accordionFlushExample">
      <div class="accordion-body">
        <p class="author">Authors: Oliveira, M. M.; Mohamed, M. S.; Elder, M. K.; Banegas-Morales, K.; Mamcarz, M.; Lu, E. H.; Golhan, E. A. N.; Navrange, N.; Chatterjee, S.; Abel, T.; Klann, E.</p>
        <p class="info">Score: 18.9, Published: 2023-07-12 </p>
        <p class="info">DOI: <a href='https://doi.org/10.1101/2023.07.12.548606' target='https://doi.org/10.1101/2023.07.12.548606'> 10.1101/2023.07.12.548606</a></p>
        <p class="abstract">Neuronal protein synthesis is required for long-lasting plasticity and long-term memory consolidation. Dephosphorylation of eukaryotic initiation factor 2 is one of the key translational control events that is required to increase de novo protein synthesis that underlies long-lasting plasticity and memory consolidation. Here, we interrogate the molecular pathways of translational control that are triggered by stimulation of neurons with brain-derived neurotrophic factor (BDNF), which results in eIF2 dephosphorylation and de novo protein synthesis. Primary rodent neurons exposed to BDNF displayed elevated translation, but not transcription, of GADD34, which facilitates eIF2 dephosphorylation and subsequent de novo protein synthesis. Furthermore, GADD34 requires G-actin generated by cofilin to dephosphorylate eIF2 and enhance protein synthesis. Finally, GADD34 is required for the BDNF-induced translation of synaptic plasticity-related proteins. Overall, we provide evidence that neurons repurpose GADD34, an effector of the Integrated Stress Response, as an orchestrator of rapid increases in eIF2-dependent translation in response to plasticity-inducing stimuli.</p>
      </div>
    </div>
  </div>
</div>









<script src="/js/bundle.min.js" defer></script>



  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2023 <a href="https://trxiv.yorks0n.com">TRxiv2</a></span>
    <span>
        · Made by Yorkson
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
