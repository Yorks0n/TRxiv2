<!DOCTYPE html>
<html lang="en" dir="auto">

<head>
<head>
    <title>health informatics</title>
    <meta charset="utf-8">
    <meta name="description"
        content="Website meta description for google search results go here" />
    <meta name="dc.relation" content="https://trxiv.yorks0n.com" />
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="theme-color" content="#1A94D2" />

    

    
    
    
    <link rel="stylesheet" href="/css/main.min.css" media="screen">

</head>


<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>health informatics | TRxiv2</title>
<meta name="keywords" content="">
<meta name="description" content="Beyond the hype: large language models propagate race-based medicine
Authors: Omiye, J. A.; Lester, J.; Spichak, S.; Rotemberg, V.; Daneshjou, R.
Score: 66.7, Published: 2023-07-05 DOI: 10.1101/2023.07.03.23292192
ImportanceLarge language models (LLMs) are being integrated into healthcare systems; but these models recapitulate harmful, race-based medicine. ObjectiveThe objective of this study is to assess whether four commercially available large language models (LLMs) propagate harmful, inaccurate, race-based content when responding to eight different scenarios that historically included race-based medicine or widespread misconceptions around race.">
<meta name="author" content="">
<link rel="canonical" href="https://trxiv.yorks0n.com/posts/health-informatics/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.904bd1e751cdd2a584fa6bed3fa1166dfd8ec9949ebfd0c4d69c5add5e17c23d.css" integrity="sha256-kEvR51HN0qWE&#43;mvtP6EWbf2OyZSev9DE1pxa3V4Xwj0=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js" integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG&#43;9vmJ0cTS&#43;ovo0FeA="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://trxiv.yorks0n.com/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://trxiv.yorks0n.com/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://trxiv.yorks0n.com/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://trxiv.yorks0n.com/apple-touch-icon.png">
<link rel="mask-icon" href="https://trxiv.yorks0n.com/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:title" content="health informatics" />
<meta property="og:description" content="Beyond the hype: large language models propagate race-based medicine
Authors: Omiye, J. A.; Lester, J.; Spichak, S.; Rotemberg, V.; Daneshjou, R.
Score: 66.7, Published: 2023-07-05 DOI: 10.1101/2023.07.03.23292192
ImportanceLarge language models (LLMs) are being integrated into healthcare systems; but these models recapitulate harmful, race-based medicine. ObjectiveThe objective of this study is to assess whether four commercially available large language models (LLMs) propagate harmful, inaccurate, race-based content when responding to eight different scenarios that historically included race-based medicine or widespread misconceptions around race." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://trxiv.yorks0n.com/posts/health-informatics/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-07-30T10:36:57+00:00" />
<meta property="article:modified_time" content="2023-07-30T10:36:57+00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="health informatics"/>
<meta name="twitter:description" content="Beyond the hype: large language models propagate race-based medicine
Authors: Omiye, J. A.; Lester, J.; Spichak, S.; Rotemberg, V.; Daneshjou, R.
Score: 66.7, Published: 2023-07-05 DOI: 10.1101/2023.07.03.23292192
ImportanceLarge language models (LLMs) are being integrated into healthcare systems; but these models recapitulate harmful, race-based medicine. ObjectiveThe objective of this study is to assess whether four commercially available large language models (LLMs) propagate harmful, inaccurate, race-based content when responding to eight different scenarios that historically included race-based medicine or widespread misconceptions around race."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Posts",
      "item": "https://trxiv.yorks0n.com/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  3 ,
      "name": "health informatics",
      "item": "https://trxiv.yorks0n.com/posts/health-informatics/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "health informatics",
  "name": "health informatics",
  "description": "Beyond the hype: large language models propagate race-based medicine\nAuthors: Omiye, J. A.; Lester, J.; Spichak, S.; Rotemberg, V.; Daneshjou, R.\nScore: 66.7, Published: 2023-07-05 DOI: 10.1101/2023.07.03.23292192\nImportanceLarge language models (LLMs) are being integrated into healthcare systems; but these models recapitulate harmful, race-based medicine. ObjectiveThe objective of this study is to assess whether four commercially available large language models (LLMs) propagate harmful, inaccurate, race-based content when responding to eight different scenarios that historically included race-based medicine or widespread misconceptions around race.",
  "keywords": [
    
  ],
  "articleBody": " Beyond the hype: large language models propagate race-based medicine\nAuthors: Omiye, J. A.; Lester, J.; Spichak, S.; Rotemberg, V.; Daneshjou, R.\nScore: 66.7, Published: 2023-07-05 DOI: 10.1101/2023.07.03.23292192\nImportanceLarge language models (LLMs) are being integrated into healthcare systems; but these models recapitulate harmful, race-based medicine. ObjectiveThe objective of this study is to assess whether four commercially available large language models (LLMs) propagate harmful, inaccurate, race-based content when responding to eight different scenarios that historically included race-based medicine or widespread misconceptions around race. Evidence ReviewQuestions were derived from discussion among 4 physician experts and prior work on race-based medical misconceptions of medical trainees. FindingsWe assessed four large language models with eight different questions that were interrogated five times each with a total of forty responses per a model. All models had examples of perpetuating race-based medicine in their responses. Models were not always consistent in their responses when asked the same question repeatedly. Conclusions and RelevanceLLMs are being proposed for use in the healthcare setting, with some models already connecting to electronic health record systems. However, this study shows that based on our findings, these LLMs could potentially cause harm by perpetuating debunked, racist concepts.\nA data-driven framework for clinical decision support applied to pneumonia management\nAuthors: Free, R. C.; Lozano Rojas, D.; Richardson, M.; Skeemer, J.; Small, L.; Haldar, P.; Woltmann, G.\nScore: 2.0, Published: 2023-07-23 DOI: 10.1101/2023.07.19.23291197\nDespite their long history, it can still be difficult to embed clinical decision support into existing health information systems, particularly if they utilise machine learning and artificial intelligence models. Moreover, when such tools are made available to healthcare workers, it is important that the users can understand and visualise the reasons for the decision support predictions. Plausibility can be hard to achieve for complex pathways and models and perceived black-box functionality often leads to a lack of trust. Here, we describe and evaluate a data-driven framework which moderates some of these issues and demonstrate its applicability to the in-hospital management of community acquired pneumonia, an acute respiratory disease which is a leading cause of in-hospital mortality world-wide. We use the framework to develop and test a clinical decision support tool based on local guideline aligned management of the disease and show how it could be used to effectively prioritise patients using retrospective analysis. Furthermore, we show how this tool can be embedded into a prototype clinical system for disease management by integrating metrics and visualisations for assisting decision makers examining complex patient journeys, risk scores and predictions from embedded machine learning and artificial intelligence models. Our results show the potential of this approach for developing, testing and evaluating workflow based clinical decision support tools which include complex models and embedding them into clinical systems.\nPheMIME: An Interactive Web App and Knowledge Base for Phenome-Wide, Multi-Institutional Multimorbidity Analysis\nAuthors: Zhang, S.; Strayer, N.; Vessels, T.; Choi, K.; Wang, G. W.; Li, Y.; Bejan, C. A.; Hsi, R. S.; Bick, A. G.; Velez Edwards, D. R.; Savona, M. R.; Philips, E. J.; Pulley, J.; Self, W. H.; Hopkins, W. C.; Roden, D. M.; Smoller, J.; Ruderfer, D. M.; Xu, Y.\nScore: 1.2, Published: 2023-07-27 DOI: 10.1101/2023.07.23.23293047\nMotivationMultimorbidity, characterized by the simultaneous occurrence of multiple diseases in an individual, is an increasing global health concern, posing substantial challenges to healthcare systems. Comprehensive understanding of disease-disease interactions and intrinsic mechanisms behind multimorbidity can offer opportunities for innovative prevention strategies, targeted interventions, and personalized treatments. Yet, there exist limited tools and datasets that characterize multimorbidity patterns across different populations. To bridge this gap, we used large-scale electronic health record (EHR) systems to develop the Phenome-wide Multi-Institutional Multimorbidity Explorer (PheMIME), which facilitates research in exploring and comparing multimorbidity patterns among multiple institutions, potentially leading to the discovery of novel and robust disease associations and patterns that are interoperable across different systems and organizations. ResultsPheMIME integrates summary statistics from phenome-wide analyses of disease multimorbidities. These are currently derived from three major institutions: Vanderbilt University Medical Center, Massachusetts General Brigham, and the UK Biobank. PheMIME offers interactive exploration of multimorbidity through multi-faceted visualization. Incorporating an enhanced version of associationSubgraphs, PheMIME enables dynamic analysis and inference of disease clusters, promoting the discovery of multimorbidity patterns. Once a disease of interest is selected, the tool generates interactive visualizations and tables that users can delve into multimorbidities or multimorbidity networks within a single system or compare across multiple systems. The utility of PheMIME is demonstrated through a case study on Schizophrenia. Availability and implementationThe PheMIME knowledge base and web application are accessible at https://prod.tbilab.org/PheMIME/. A comprehensive tutorial, including a use-case example, is available at https://prod.tbilab.org/PheMIME_supplementary_materials/. Furthermore, the source code for PheMIME can be freely downloaded from https://github.com/tbilab/PheMIME. Data availability statementThe data underlying this article are available in the article and in its online web application or supplementary material.\nCoding Inequity: Assessing GPT-4's Potential for Perpetuating Racial and Gender Biases in Healthcare\nAuthors: Zack, T.; Lehman, E.; Suzgun, M.; Rodriguez, J. A.; Celi, L. A.; Gichoya, J.; Jurafsky, D.; Szolovits, P.; Bates, D. W.; Abdulnour, R.-E. E.; Butte, A. J.; Alsentzer, E.\nScore: 10.8, Published: 2023-07-17 DOI: 10.1101/2023.07.13.23292577\nBackgroundLarge language models (LLMs) such as GPT-4 hold great promise as transformative tools in healthcare, ranging from automating administrative tasks to augmenting clinical decision- making. However, these models also pose a serious danger of perpetuating biases and delivering incorrect medical diagnoses, which can have a direct, harmful impact on medical care. MethodsUsing the Azure OpenAI API, we tested whether GPT-4 encodes racial and gender biases and examined the impact of such biases on four potential applications of LLMs in the clinical domain--namely, medical education, diagnostic reasoning, plan generation, and patient assessment. We conducted experiments with prompts designed to resemble typical use of GPT-4 within clinical and medical education applications. We used clinical vignettes from NEJM Healer and from published research on implicit bias in healthcare. GPT-4 estimates of the demographic distribution of medical conditions were compared to true U.S. prevalence estimates. Differential diagnosis and treatment planning were evaluated across demographic groups using standard statistical tests for significance between groups. FindingsWe find that GPT-4 does not appropriately model the demographic diversity of medical conditions, consistently producing clinical vignettes that stereotype demographic presentations. The differential diagnoses created by GPT-4 for standardized clinical vignettes were more likely to include diagnoses that stereotype certain races, ethnicities, and gender identities. Assessment and plans created by the model showed significant association between demographic attributes and recommendations for more expensive procedures as well as differences in patient perception. InterpretationOur findings highlight the urgent need for comprehensive and transparent bias assessments of LLM tools like GPT-4 for every intended use case before they are integrated into clinical care. We discuss the potential sources of these biases and potential mitigation strategies prior to clinical implementation.\nOn the limitations of large language models in clinical diagnosis\nAuthors: Reese, J.; Danis, D.; Caufield, J. H.; Casiraghi, E.; Valentini, G.; Mungall, C. J.; Robinson, P. N.\nScore: 3.1, Published: 2023-07-14 DOI: 10.1101/2023.07.13.23292613\nBackgroundThe potential of large language models (LLM) such as GPT to support complex tasks such as differential diagnosis has been a subject of debate, with some ascribing near sentient abilities to the models and others claiming that LLMs merely perform \"autocomplete on steroids\". A recent study reported that the Generative Pretrained Transformer 4 (GPT-4) model performed well in complex differential diagnostic reasoning. The authors assessed the performance of GPT-4 in identifying the correct diagnosis in a series of case records from the New England Journal of Medicine. The authors constructed prompts based on the clinical presentation section of the case reports, and compared the results of GPT-4 to the actual diagnosis. GPT-4 returned the correct diagnosis as a part of its response in 64% of cases, with the correct diagnosis being at rank 1 in 39% of cases. However, such concise but comprehensive narratives of the clinical course are not typically available in electronic health records (EHRs). Further, if they were available, EHR records contain identifying information whose transmission is prohibited by Health Insurance Portability and Accountability Act (HIPAA) regulations. MethodsTo assess the expected performance of GPT on comparable datasets that can be generated by text mining and by design cannot contain identifiable information, we parsed the texts of the case reports and extracted Human Phenotype Ontology (HPO) terms, from which prompts for GPT were constructed that contain largely the same clinical abnormalities but lack the surrounding narrative. ResultsWhile the performance of GPT-4 on the original narrative-based text was good, with the final diagnosis being included in its differential in 29/75 cases (38.7%; rank 1 in 17.3% of cases; mean rank of 3.4), the performance of GPT-4 on the feature-based approach that includes the major clinical abnormalities without additional narrative texas substantially worse, with GPT-4 including the final diagnosis in its differential in 8/75 cases (10.7%; rank 1 in 4.0% of cases; mean rank of 3.9). InterpretationWe consider the feature-based queries to be a more appropriate test of the performance of GPT-4 in diagnostic tasks, since it is unlikely that the narrative approach can be used in actual clinical practice. Future research and algorithmic development is needed to determine the optimal approach to leveraging LLMs for clinical diagnosis.\n",
  "wordCount" : "1521",
  "inLanguage": "en",
  "datePublished": "2023-07-30T10:36:57Z",
  "dateModified": "2023-07-30T10:36:57Z",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://trxiv.yorks0n.com/posts/health-informatics/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "TRxiv2",
    "logo": {
      "@type": "ImageObject",
      "url": "https://trxiv.yorks0n.com/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://trxiv.yorks0n.com" accesskey="h" title="TRxiv2 (Alt + H)">TRxiv2</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
        </ul>
    </nav>
</header>
<main class="main">
<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title">
      health informatics
    </h1>
    <div class="post-meta"><span>updated on July 30, 2023</span>

</div>
  </header> 
  <div class="post-content"><div class="accordion accordion-flush" id="accordionFlushExample"><div class="accordion-item">
    <h3 class="accordion-header" id="flush-heading10.1101/2023.07.03.23292192">
      <button class="accordion-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapse10.1101/2023.07.03.23292192" aria-expanded="false" aria-controls="flush-collapse10.1101/2023.07.03.23292192">
        <p class="paperTitle">Beyond the hype: large language models propagate race-based medicine</p>
      </button>
    </h3>
    <div id="flush-collapse10.1101/2023.07.03.23292192" class="accordion-collapse collapse" aria-labelledby="flush-heading10.1101/2023.07.03.23292192" data-bs-parent="#accordionFlushExample">
      <div class="accordion-body">
        <p class="author">Authors: Omiye, J. A.; Lester, J.; Spichak, S.; Rotemberg, V.; Daneshjou, R.</p>
        <p class="info">Score: 66.7, Published: 2023-07-05 </p>
        <p class="info">DOI: <a href='https://doi.org/10.1101/2023.07.03.23292192' target='https://doi.org/10.1101/2023.07.03.23292192'> 10.1101/2023.07.03.23292192</a></p>
        <p class="abstract">ImportanceLarge language models (LLMs) are being integrated into healthcare systems; but these models recapitulate harmful, race-based medicine.

ObjectiveThe objective of this study is to assess whether four commercially available large language models (LLMs) propagate harmful, inaccurate, race-based content when responding to eight different scenarios that historically included race-based medicine or widespread misconceptions around race.

Evidence ReviewQuestions were derived from discussion among 4 physician experts and prior work on race-based medical misconceptions of medical trainees.

FindingsWe assessed four large language models with eight different questions that were interrogated five times each with a total of forty responses per a model. All models had examples of perpetuating race-based medicine in their responses. Models were not always consistent in their responses when asked the same question repeatedly.

Conclusions and RelevanceLLMs are being proposed for use in the healthcare setting, with some models already connecting to electronic health record systems. However, this study shows that based on our findings, these LLMs could potentially cause harm by perpetuating debunked, racist concepts.</p>
      </div>
    </div>
  </div><div class="accordion-item">
    <h3 class="accordion-header" id="flush-heading10.1101/2023.07.19.23291197">
      <button class="accordion-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapse10.1101/2023.07.19.23291197" aria-expanded="false" aria-controls="flush-collapse10.1101/2023.07.19.23291197">
        <p class="paperTitle">A data-driven framework for clinical decision support applied to pneumonia management</p>
      </button>
    </h3>
    <div id="flush-collapse10.1101/2023.07.19.23291197" class="accordion-collapse collapse" aria-labelledby="flush-heading10.1101/2023.07.19.23291197" data-bs-parent="#accordionFlushExample">
      <div class="accordion-body">
        <p class="author">Authors: Free, R. C.; Lozano Rojas, D.; Richardson, M.; Skeemer, J.; Small, L.; Haldar, P.; Woltmann, G.</p>
        <p class="info">Score: 2.0, Published: 2023-07-23 </p>
        <p class="info">DOI: <a href='https://doi.org/10.1101/2023.07.19.23291197' target='https://doi.org/10.1101/2023.07.19.23291197'> 10.1101/2023.07.19.23291197</a></p>
        <p class="abstract">Despite their long history, it can still be difficult to embed clinical decision support into existing health information systems, particularly if they utilise machine learning and artificial intelligence models. Moreover, when such tools are made available to healthcare workers, it is important that the users can understand and visualise the reasons for the decision support predictions. Plausibility can be hard to achieve for complex pathways and models and perceived black-box functionality often leads to a lack of trust. Here, we describe and evaluate a data-driven framework which moderates some of these issues and demonstrate its applicability to the in-hospital management of community acquired pneumonia, an acute respiratory disease which is a leading cause of in-hospital mortality world-wide. We use the framework to develop and test a clinical decision support tool based on local guideline aligned management of the disease and show how it could be used to effectively prioritise patients using retrospective analysis. Furthermore, we show how this tool can be embedded into a prototype clinical system for disease management by integrating metrics and visualisations for assisting decision makers examining complex patient journeys, risk scores and predictions from embedded machine learning and artificial intelligence models. Our results show the potential of this approach for developing, testing and evaluating workflow based clinical decision support tools which include complex models and embedding them into clinical systems.</p>
      </div>
    </div>
  </div><div class="accordion-item">
    <h3 class="accordion-header" id="flush-heading10.1101/2023.07.23.23293047">
      <button class="accordion-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapse10.1101/2023.07.23.23293047" aria-expanded="false" aria-controls="flush-collapse10.1101/2023.07.23.23293047">
        <p class="paperTitle">PheMIME: An Interactive Web App and Knowledge Base for Phenome-Wide, Multi-Institutional Multimorbidity Analysis</p>
      </button>
    </h3>
    <div id="flush-collapse10.1101/2023.07.23.23293047" class="accordion-collapse collapse" aria-labelledby="flush-heading10.1101/2023.07.23.23293047" data-bs-parent="#accordionFlushExample">
      <div class="accordion-body">
        <p class="author">Authors: Zhang, S.; Strayer, N.; Vessels, T.; Choi, K.; Wang, G. W.; Li, Y.; Bejan, C. A.; Hsi, R. S.; Bick, A. G.; Velez Edwards, D. R.; Savona, M. R.; Philips, E. J.; Pulley, J.; Self, W. H.; Hopkins, W. C.; Roden, D. M.; Smoller, J.; Ruderfer, D. M.; Xu, Y.</p>
        <p class="info">Score: 1.2, Published: 2023-07-27 </p>
        <p class="info">DOI: <a href='https://doi.org/10.1101/2023.07.23.23293047' target='https://doi.org/10.1101/2023.07.23.23293047'> 10.1101/2023.07.23.23293047</a></p>
        <p class="abstract">MotivationMultimorbidity, characterized by the simultaneous occurrence of multiple diseases in an individual, is an increasing global health concern, posing substantial challenges to healthcare systems. Comprehensive understanding of disease-disease interactions and intrinsic mechanisms behind multimorbidity can offer opportunities for innovative prevention strategies, targeted interventions, and personalized treatments. Yet, there exist limited tools and datasets that characterize multimorbidity patterns across different populations. To bridge this gap, we used large-scale electronic health record (EHR) systems to develop the Phenome-wide Multi-Institutional Multimorbidity Explorer (PheMIME), which facilitates research in exploring and comparing multimorbidity patterns among multiple institutions, potentially leading to the discovery of novel and robust disease associations and patterns that are interoperable across different systems and organizations.

ResultsPheMIME integrates summary statistics from phenome-wide analyses of disease multimorbidities. These are currently derived from three major institutions: Vanderbilt University Medical Center, Massachusetts General Brigham, and the UK Biobank. PheMIME offers interactive exploration of multimorbidity through multi-faceted visualization. Incorporating an enhanced version of associationSubgraphs, PheMIME enables dynamic analysis and inference of disease clusters, promoting the discovery of multimorbidity patterns. Once a disease of interest is selected, the tool generates interactive visualizations and tables that users can delve into multimorbidities or multimorbidity networks within a single system or compare across multiple systems. The utility of PheMIME is demonstrated through a case study on Schizophrenia.

Availability and implementationThe PheMIME knowledge base and web application are accessible at https://prod.tbilab.org/PheMIME/. A comprehensive tutorial, including a use-case example, is available at https://prod.tbilab.org/PheMIME_supplementary_materials/. Furthermore, the source code for PheMIME can be freely downloaded from https://github.com/tbilab/PheMIME.

Data availability statementThe data underlying this article are available in the article and in its online web application or supplementary material.</p>
      </div>
    </div>
  </div><div class="accordion-item">
    <h3 class="accordion-header" id="flush-heading10.1101/2023.07.13.23292577">
      <button class="accordion-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapse10.1101/2023.07.13.23292577" aria-expanded="false" aria-controls="flush-collapse10.1101/2023.07.13.23292577">
        <p class="paperTitle">Coding Inequity: Assessing GPT-4&#39;s Potential for Perpetuating Racial and Gender Biases in Healthcare</p>
      </button>
    </h3>
    <div id="flush-collapse10.1101/2023.07.13.23292577" class="accordion-collapse collapse" aria-labelledby="flush-heading10.1101/2023.07.13.23292577" data-bs-parent="#accordionFlushExample">
      <div class="accordion-body">
        <p class="author">Authors: Zack, T.; Lehman, E.; Suzgun, M.; Rodriguez, J. A.; Celi, L. A.; Gichoya, J.; Jurafsky, D.; Szolovits, P.; Bates, D. W.; Abdulnour, R.-E. E.; Butte, A. J.; Alsentzer, E.</p>
        <p class="info">Score: 10.8, Published: 2023-07-17 </p>
        <p class="info">DOI: <a href='https://doi.org/10.1101/2023.07.13.23292577' target='https://doi.org/10.1101/2023.07.13.23292577'> 10.1101/2023.07.13.23292577</a></p>
        <p class="abstract">BackgroundLarge language models (LLMs) such as GPT-4 hold great promise as transformative tools in healthcare, ranging from automating administrative tasks to augmenting clinical decision- making. However, these models also pose a serious danger of perpetuating biases and delivering incorrect medical diagnoses, which can have a direct, harmful impact on medical care.

MethodsUsing the Azure OpenAI API, we tested whether GPT-4 encodes racial and gender biases and examined the impact of such biases on four potential applications of LLMs in the clinical domain--namely, medical education, diagnostic reasoning, plan generation, and patient assessment. We conducted experiments with prompts designed to resemble typical use of GPT-4 within clinical and medical education applications. We used clinical vignettes from NEJM Healer and from published research on implicit bias in healthcare. GPT-4 estimates of the demographic distribution of medical conditions were compared to true U.S. prevalence estimates. Differential diagnosis and treatment planning were evaluated across demographic groups using standard statistical tests for significance between groups.

FindingsWe find that GPT-4 does not appropriately model the demographic diversity of medical conditions, consistently producing clinical vignettes that stereotype demographic presentations. The differential diagnoses created by GPT-4 for standardized clinical vignettes were more likely to include diagnoses that stereotype certain races, ethnicities, and gender identities. Assessment and plans created by the model showed significant association between demographic attributes and recommendations for more expensive procedures as well as differences in patient perception.

InterpretationOur findings highlight the urgent need for comprehensive and transparent bias assessments of LLM tools like GPT-4 for every intended use case before they are integrated into clinical care. We discuss the potential sources of these biases and potential mitigation strategies prior to clinical implementation.</p>
      </div>
    </div>
  </div><div class="accordion-item">
    <h3 class="accordion-header" id="flush-heading10.1101/2023.07.13.23292613">
      <button class="accordion-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapse10.1101/2023.07.13.23292613" aria-expanded="false" aria-controls="flush-collapse10.1101/2023.07.13.23292613">
        <p class="paperTitle">On the limitations of large language models in clinical diagnosis</p>
      </button>
    </h3>
    <div id="flush-collapse10.1101/2023.07.13.23292613" class="accordion-collapse collapse" aria-labelledby="flush-heading10.1101/2023.07.13.23292613" data-bs-parent="#accordionFlushExample">
      <div class="accordion-body">
        <p class="author">Authors: Reese, J.; Danis, D.; Caufield, J. H.; Casiraghi, E.; Valentini, G.; Mungall, C. J.; Robinson, P. N.</p>
        <p class="info">Score: 3.1, Published: 2023-07-14 </p>
        <p class="info">DOI: <a href='https://doi.org/10.1101/2023.07.13.23292613' target='https://doi.org/10.1101/2023.07.13.23292613'> 10.1101/2023.07.13.23292613</a></p>
        <p class="abstract">BackgroundThe potential of large language models (LLM) such as GPT to support complex tasks such as differential diagnosis has been a subject of debate, with some ascribing near sentient abilities to the models and others claiming that LLMs merely perform &#34;autocomplete on steroids&#34;. A recent study reported that the Generative Pretrained Transformer 4 (GPT-4) model performed well in complex differential diagnostic reasoning. The authors assessed the performance of GPT-4 in identifying the correct diagnosis in a series of case records from the New England Journal of Medicine. The authors constructed prompts based on the clinical presentation section of the case reports, and compared the results of GPT-4 to the actual diagnosis. GPT-4 returned the correct diagnosis as a part of its response in 64% of cases, with the correct diagnosis being at rank 1 in 39% of cases. However, such concise but comprehensive narratives of the clinical course are not typically available in electronic health records (EHRs). Further, if they were available, EHR records contain identifying information whose transmission is prohibited by Health Insurance Portability and Accountability Act (HIPAA) regulations.

MethodsTo assess the expected performance of GPT on comparable datasets that can be generated by text mining and by design cannot contain identifiable information, we parsed the texts of the case reports and extracted Human Phenotype Ontology (HPO) terms, from which prompts for GPT were constructed that contain largely the same clinical abnormalities but lack the surrounding narrative.

ResultsWhile the performance of GPT-4 on the original narrative-based text was good, with the final diagnosis being included in its differential in 29/75 cases (38.7%; rank 1 in 17.3% of cases; mean rank of 3.4), the performance of GPT-4 on the feature-based approach that includes the major clinical abnormalities without additional narrative texas substantially worse, with GPT-4 including the final diagnosis in its differential in 8/75 cases (10.7%; rank 1 in 4.0% of cases; mean rank of 3.9).

InterpretationWe consider the feature-based queries to be a more appropriate test of the performance of GPT-4 in diagnostic tasks, since it is unlikely that the narrative approach can be used in actual clinical practice. Future research and algorithmic development is needed to determine the optimal approach to leveraging LLMs for clinical diagnosis.</p>
      </div>
    </div>
  </div>
</div>









<script src="/js/bundle.min.js" defer></script>



  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2023 <a href="https://trxiv.yorks0n.com">TRxiv2</a></span>
    <span>
        Â· Made by Yorkson
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
